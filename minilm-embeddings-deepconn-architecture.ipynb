{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import os, sys, time, math, json, inspect, argparse, random, re\n",
    "from pathlib import Path\n",
    "\n",
    "# PyData / ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenization (no extra downloads required)\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Hugging Face datasets\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Please install 'datasets' first, e.g. `pip install datasets`\"\n",
    "    ) from e\n",
    "\n",
    "# ---- Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IN_KAGGLE = Path(\"/kaggle\").exists()\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "    WORK_ROOT  = Path(\"/kaggle/working\")\n",
    "\n",
    "    STOPWORDS = INPUT_ROOT / \"digital-music-5\" / \"stopwords.txt\"\n",
    "    PUNCTS    = INPUT_ROOT / \"digital-music-5\" / \"punctuations.txt\"\n",
    "    GLOVE     = INPUT_ROOT / \"glove6b100dtxt\" / \"glove.6B.100d.txt\"\n",
    "\n",
    "    RAW_ALL_BEAUTY = INPUT_ROOT / \"all-beauty\" / \"All_Beauty.jsonl\"\n",
    "    LOCAL_ST_MODEL = INPUT_ROOT / \"minilm-l6-v2-local\" / \"all-MiniLM-L6-v2\"\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "    # if the folders are directly under the project root:\n",
    "    RAW_ALL_BEAUTY = PROJECT_ROOT / \"All_Beauty\" / \"All_Beauty.jsonl\"\n",
    "    GLOVE          = PROJECT_ROOT / \"glove.6B.100d.txt\" / \"glove.6B.100d.txt\"\n",
    "    STOPWORDS      = PROJECT_ROOT / \"Digital_Music_5\" / \"stopwords.txt\"\n",
    "    PUNCTS         = PROJECT_ROOT / \"Digital_Music_5\" / \"punctuations.txt\"\n",
    "    # pointing to the directory that contains config.json, tokenizer.json, etc.\n",
    "    LOCAL_ST_MODEL = PROJECT_ROOT / \"all-MiniLM-L6-v2\" / \"all-MiniLM-L6-v2\"\n",
    "\n",
    "    ARTIFACTS = PROJECT_ROOT / \"artifacts\"\n",
    "\n",
    "# Output directories as Path\n",
    "WORK_DIR      = WORK_ROOT if IN_KAGGLE else ARTIFACTS\n",
    "SAVE_DATA_DIR = WORK_DIR / \"Amazon_Fashion\"\n",
    "MODEL_DIR     = WORK_DIR / \"model\"\n",
    "FIG_DIR       = WORK_DIR / \"fig\"\n",
    "ST_CACHE_DIR  = WORK_DIR / \"st_cache\"\n",
    "\n",
    "# Create directories\n",
    "for d in [SAVE_DATA_DIR, MODEL_DIR, FIG_DIR, ST_CACHE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [RAW_ALL_BEAUTY, GLOVE, STOPWORDS, PUNCTS, LOCAL_ST_MODEL]:\n",
    "    print(p, \"exists:\", p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset  # optional fallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "def load_all_beauty_local(jsonl_path: str | Path | None = None):\n",
    "    \"\"\"\n",
    "    Load All_Beauty.jsonl (Kaggle or local) and return a pandas DataFrame\n",
    "    with standardized columns: userID, itemID, review, rating.\n",
    "    \"\"\"\n",
    "    if jsonl_path is None:\n",
    "        jsonl_path = RAW_ALL_BEAUTY\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "\n",
    "    needed = (\"user_id\", \"asin\", \"text\", \"rating\")\n",
    "\n",
    "    def _standardize_cols(df):\n",
    "        alt_map = {\n",
    "            \"reviewText\": \"text\",\n",
    "            \"overall\": \"rating\",\n",
    "            \"user\": \"user_id\",\n",
    "            \"item\": \"asin\",\n",
    "        }\n",
    "        for old, new in alt_map.items():\n",
    "            if old in df.columns and new not in df.columns:\n",
    "                df[new] = df[old]\n",
    "\n",
    "        missing = [c for c in needed if c not in df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"Missing required columns {missing}. \"\n",
    "                \"Make sure your JSONL has keys like: user_id, asin, text, rating.\"\n",
    "            )\n",
    "\n",
    "        df = df[list(needed)].copy()\n",
    "        df.columns = [\"userID\", \"itemID\", \"review\", \"rating\"]\n",
    "        df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "        df = df[df[\"rating\"].notnull()]\n",
    "        df = df[df[\"review\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        df = pd.read_json(jsonl_path, lines=True)\n",
    "        return _standardize_cols(df)\n",
    "    except Exception as e_pd:\n",
    "        try:\n",
    "            ds = load_dataset(\"json\", data_files=str(jsonl_path), split=\"train\")\n",
    "            df = ds.to_pandas()\n",
    "            return _standardize_cols(df)\n",
    "        except Exception as e_hf:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load JSONL via pandas ({type(e_pd).__name__}: {e_pd}) \"\n",
    "                f\"and datasets ({type(e_hf).__name__}: {e_hf}).\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- keep your helpers as-is ---\n",
    "def _read_list(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Required file not found: {path}\\n\"\n",
    "            \"Place a plain-text file with one token per line.\"\n",
    "        )\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return set(ln.strip() for ln in f if ln.strip())\n",
    "\n",
    "def process_df_to_csv(df, stopwords_path, puncts_path, train_rate, csv_path):\n",
    "    # Map IDs to contiguous integers\n",
    "    df[\"userID\"] = df[\"userID\"].astype(\"category\").cat.codes\n",
    "    df[\"itemID\"] = df[\"itemID\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # Load stopwords/punctuations\n",
    "    stop_words   = _read_list(stopwords_path)\n",
    "    punctuations = _read_list(puncts_path)\n",
    "    tok = WordPunctTokenizer()\n",
    "\n",
    "    def clean_review(review: str) -> str:\n",
    "        rv = review.lower()\n",
    "        for p in punctuations:\n",
    "            rv = rv.replace(p, \" \")\n",
    "        toks = tok.tokenize(rv)\n",
    "        toks = [w for w in toks if w not in stop_words]\n",
    "        return \" \".join(toks)\n",
    "\n",
    "    print(\"#### Cleaning text (this can take a while on large splits)...\")\n",
    "    df[\"review\"] = df[\"review\"].apply(clean_review)\n",
    "\n",
    "    # Train/valid/test split\n",
    "    train_df, valid_test_df = train_test_split(df, test_size=1 - train_rate, random_state=3)\n",
    "    valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=4)\n",
    "\n",
    "    os.makedirs(csv_path, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(csv_path, \"train.csv\"), index=False, header=False)\n",
    "    valid_df.to_csv(os.path.join(csv_path, \"valid.csv\"), index=False, header=False)\n",
    "    test_df .to_csv(os.path.join(csv_path, \"test.csv\"),  index=False, header=False)\n",
    "\n",
    "    print(f\"#### Saved CSVs to {csv_path}\")\n",
    "    print(f\"#### Split sizes: train {len(train_df)}, valid {len(valid_df)}, test {len(test_df)}\")\n",
    "    print(f\"#### Totals: {len(df)} reviews, {df['userID'].nunique()} users, {df['itemID'].nunique()} items.\")\n",
    "    return train_df, valid_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train = SAVE_DATA_DIR / \"train.csv\"\n",
    "\n",
    "if not csv_train.exists():\n",
    "    df_raw = load_all_beauty_local()  # uses Kaggle or local automatically\n",
    "    _ = process_df_to_csv(\n",
    "        df_raw,\n",
    "        stopwords_path=STOPWORDS,\n",
    "        puncts_path=PUNCTS,\n",
    "        train_rate=0.8,\n",
    "        csv_path=SAVE_DATA_DIR,\n",
    "    )\n",
    "else:\n",
    "    print(\"CSV files already exist — skipping reprocessing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(f='%Y-%m-%d %H:%M:%S'):\n",
    "    return time.strftime(f, time.localtime())\n",
    "\n",
    "class Config:\n",
    "    # Device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Training\n",
    "    train_epochs        = 5\n",
    "    batch_size          = 128\n",
    "    learning_rate       = 2e-3\n",
    "    l2_regularization   = 1e-6\n",
    "    learning_rate_decay = 0.99\n",
    "    patience            = 3\n",
    "\n",
    "    # Files\n",
    "    word2vec_file = GLOVE\n",
    "    train_file    = os.path.join(SAVE_DATA_DIR, 'train.csv')\n",
    "    valid_file    = os.path.join(SAVE_DATA_DIR, 'valid.csv')\n",
    "    test_file     = os.path.join(SAVE_DATA_DIR, 'test.csv')\n",
    "    model_file    = os.path.join(MODEL_DIR, 'best_model.pt')\n",
    "\n",
    "    # Data shaping\n",
    "    review_count         = 10    # number of reviews per side\n",
    "    review_length        = 40    # tokens per review\n",
    "    lowest_review_count  = 2\n",
    "    PAD_WORD             = '<UNK>'\n",
    "\n",
    "    # Model sizes\n",
    "    kernel_count = 100\n",
    "    kernel_size  = 3\n",
    "    dropout_prob = 0.5\n",
    "    cnn_out_dim  = 50\n",
    "\n",
    "    def __init__(self):\n",
    "        # Allow CLI/nb override (no-op by default)\n",
    "        attributes = inspect.getmembers(self, lambda a: not inspect.isfunction(a))\n",
    "        attributes = list(filter(lambda x: not x[0].startswith('__'), attributes))\n",
    "        parser = argparse.ArgumentParser(add_help=False)\n",
    "        for key, val in attributes:\n",
    "            parser.add_argument('--' + key, dest=key, type=type(val), default=val)\n",
    "        args, _ = parser.parse_known_args([])\n",
    "        for key, val in args.__dict__.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "    def __str__(self):\n",
    "        attributes = inspect.getmembers(self, lambda a: not inspect.isfunction(a))\n",
    "        attributes = list(filter(lambda x: not x[0].startswith('__'), attributes))\n",
    "        return \"\\n\".join([f\"{k} = {v}\" for k, v in attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & small utils\n",
    "import os, json, re, csv, hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def simple_sentence_split(text: str, max_sentences: int):\n",
    "    \"\"\"Split on . ! ? and keep up to max_sentences.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sents = [s.strip() for s in parts if s.strip()]\n",
    "    return sents[:max_sentences]\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split(csv_path: str):\n",
    "    \"\"\"Reads CSV with no header: userID,itemID,review,rating\"\"\"\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.reader(f)\n",
    "        for user_id, item_id, review, rating in rdr:\n",
    "            rows.append((int(user_id), int(item_id), review, float(rating)))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_sent_embeddings(\n",
    "    train_csv: str,\n",
    "    valid_csv: str,\n",
    "    test_csv: str,\n",
    "    out_dir: str,\n",
    "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    S: int = 40,\n",
    "    batch_size: int = 1024,\n",
    "    max_len: int = 128,\n",
    "    lowercase: bool = False,\n",
    "    local_model_dir: str | None = \"/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2\",  # <-- your local path\n",
    "):\n",
    "    \"\"\"\n",
    "    Notebook version of precompute_sent_embeddings.py\n",
    "    Writes:\n",
    "      out_dir/\n",
    "        embeddings.npy\n",
    "        sentences.jsonl\n",
    "        review2sent_ids.jsonl\n",
    "        splits/{train,valid,test}/ (rows.jsonl, user_reviews.json, item_reviews.json)\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    ensure_dir(out_dir)\n",
    "    splits_dir = out_dir / \"splits\"\n",
    "    ensure_dir(splits_dir)\n",
    "\n",
    "    # 1) Read all splits\n",
    "    split_paths = {\"train\": Path(train_csv), \"valid\": Path(valid_csv), \"test\": Path(test_csv)}\n",
    "    splits = {k: read_split(str(v)) for k, v in split_paths.items()}\n",
    "\n",
    "    # 2) Collect unique reviews + per-split manifests\n",
    "    all_reviews = {}   # review_hash -> raw_review_text\n",
    "    split_rows = {}    # split -> list[dict]\n",
    "    user_reviews = {}  # split -> {userID: [review_hash,...]}\n",
    "    item_reviews = {}  # split -> {itemID: [review_hash,...]}\n",
    "\n",
    "    for split, rows in splits.items():\n",
    "        s_rows = []\n",
    "        u_map = defaultdict(list)\n",
    "        i_map = defaultdict(list)\n",
    "        for (u, it, review, rating) in rows:\n",
    "            rtxt = review.lower() if lowercase else review\n",
    "            r_hash = sha1(rtxt)\n",
    "            all_reviews.setdefault(r_hash, rtxt)\n",
    "            s_rows.append({\"userID\": u, \"itemID\": it, \"rating\": rating, \"review_hash\": r_hash})\n",
    "            u_map[u].append(r_hash)\n",
    "            i_map[it].append(r_hash)\n",
    "        split_rows[split] = s_rows\n",
    "        user_reviews[split] = {str(k): v for k, v in u_map.items()}\n",
    "        item_reviews[split] = {str(k): v for k, v in i_map.items()}\n",
    "\n",
    "    # 3) Build deduped sentence inventory (cap to S per review for downstream speed)\n",
    "    sentence_to_id = {}\n",
    "    sentences = []  # index -> text\n",
    "    review_to_sentids = {}  # review_hash -> fixed-length [int] of len S, with -1 as PAD\n",
    "\n",
    "    def get_sent_id(s):\n",
    "        if s not in sentence_to_id:\n",
    "            sentence_to_id[s] = len(sentences)\n",
    "            sentences.append(s)\n",
    "        return sentence_to_id[s]\n",
    "\n",
    "    for r_hash, rtxt in all_reviews.items():\n",
    "        sents = simple_sentence_split(rtxt, S)\n",
    "        ids = [get_sent_id(s) for s in sents]\n",
    "        ids = (ids + [-1] * (S - len(ids))) if len(ids) < S else ids[:S]\n",
    "        review_to_sentids[r_hash] = ids\n",
    "\n",
    "    print(f\"[build] Unique reviews: {len(all_reviews):,}\")\n",
    "    print(f\"[build] Unique sentences: {len(sentences):,}\")\n",
    "\n",
    "    # 4) Encode all unique sentences (with progress bar)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Prefer local directory (offline-safe)\n",
    "    try:\n",
    "        if local_model_dir is None:\n",
    "            raise FileNotFoundError(\"No local_model_dir provided.\")\n",
    "        # quick sanity: require at least model + tokenizer files\n",
    "        needed = [\"tokenizer.json\", \"special_tokens_map.json\"]\n",
    "        missing = [f for f in needed if not os.path.exists(os.path.join(local_model_dir, f))]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"Missing in local model dir: {missing}\")\n",
    "        tok = AutoTokenizer.from_pretrained(local_model_dir, local_files_only=True)\n",
    "        enc = AutoModel.from_pretrained(local_model_dir, local_files_only=True).to(device)\n",
    "    except Exception as e:\n",
    "        # Optional: fallback to hub if you ever run with internet\n",
    "        print(\"[warn] Local model load failed, attempting hub:\", e)\n",
    "        tok = AutoTokenizer.from_pretrained(model_name)\n",
    "        enc = AutoModel.from_pretrained(model_name).to(device)\n",
    "   \n",
    "    \n",
    "    enc.eval()\n",
    "    for p in enc.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    H = enc.config.hidden_size\n",
    "    N = len(sentences)\n",
    "    EMB = np.memmap(out_dir / \"embeddings.npy\", dtype=\"float32\", mode=\"w+\", shape=(N, H))\n",
    "\n",
    "    def mean_pool(last_hidden_state, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        counts = mask.sum(dim=1).clamp(min=1.0)\n",
    "        return summed / counts\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rng = range(0, N, batch_size)\n",
    "        for start in tqdm(rng, desc=\"Encoding sentences\", unit=\"batch\"):\n",
    "            end = min(start + batch_size, N)\n",
    "            batch_texts = sentences[start:end]\n",
    "            batch = tok(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            out = enc(**batch)\n",
    "            pooled = mean_pool(out.last_hidden_state, batch[\"attention_mask\"]).detach().cpu().numpy().astype(\"float32\")\n",
    "            EMB[start:end, :] = pooled\n",
    "\n",
    "    # Flush memmap\n",
    "    del EMB\n",
    "    print(\"[encode] embeddings.npy written.\")\n",
    "\n",
    "    # 5) Write metadata files\n",
    "    with open(out_dir / \"sentences.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sid, txt in enumerate(sentences):\n",
    "            f.write(json.dumps({\"sent_id\": sid, \"text\": txt}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(out_dir / \"review2sent_ids.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for r_hash, ids in review_to_sentids.items():\n",
    "            f.write(json.dumps({\"review_hash\": r_hash, \"sent_ids\": ids}) + \"\\n\")\n",
    "\n",
    "    # Per-split artifacts\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        sp = splits_dir / split\n",
    "        ensure_dir(sp)\n",
    "        with open(sp / \"rows.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in split_rows[split]:\n",
    "                f.write(json.dumps(row) + \"\\n\")\n",
    "        with open(sp / \"user_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(user_reviews[split], f)\n",
    "        with open(sp / \"item_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(item_reviews[split], f)\n",
    "\n",
    "    print(f\"[done] Wrote cache to: {out_dir.resolve()}\")\n",
    "    print(\"      Files:\")\n",
    "    print(\"       - embeddings.npy\")\n",
    "    print(\"       - sentences.jsonl\")\n",
    "    print(\"       - review2sent_ids.jsonl\")\n",
    "    print(\"       - splits/*/rows.jsonl, user_reviews.json, item_reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = SAVE_DATA_DIR / \"train.csv\"\n",
    "VALID_CSV = SAVE_DATA_DIR / \"valid.csv\"\n",
    "TEST_CSV  = SAVE_DATA_DIR / \"test.csv\"\n",
    "OUT_DIR   = ST_CACHE_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_sent_embeddings(\n",
    "    train_csv=str(TRAIN_CSV),\n",
    "    valid_csv=str(VALID_CSV),\n",
    "    test_csv=str(TEST_CSV),\n",
    "    out_dir=str(OUT_DIR),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    S=40,\n",
    "    batch_size=1024,\n",
    "    max_len=128,\n",
    "    lowercase=True,\n",
    "    local_model_dir=str(LOCAL_ST_MODEL) if LOCAL_ST_MODEL is not None else None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: STCachedDataset (fixed rc/S) for fast numeric I/O training\n",
    "import json, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class STCachedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads numeric cache produced by precompute_sent_embeddings (fixed rc/S view).\n",
    "    - embeddings.npy  -> (N_sentences, H) float32 (memmap)\n",
    "    - review2sent_ids.jsonl -> review_hash -> [S] sentence ids (-1 pad)\n",
    "    - splits/{split}/rows.jsonl\n",
    "    - splits/{split}/user_reviews.json, item_reviews.json\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir, split, rc=10, S=40):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.split = split\n",
    "        self.rc = rc\n",
    "        self.S = S\n",
    "\n",
    "        # embeddings\n",
    "        self.emb = np.memmap(self.cache_dir / \"embeddings.npy\", dtype=\"float32\", mode=\"r\")\n",
    "        # discover H\n",
    "        N = self.emb.size\n",
    "        with open(self.cache_dir / \"sentences.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            n_sent = sum(1 for _ in f)\n",
    "        H = N // n_sent\n",
    "        self.emb = self.emb.reshape(n_sent, H)\n",
    "        self.H = H\n",
    "\n",
    "        # review -> sent_ids\n",
    "        self.rev2ids = {}\n",
    "        with open(self.cache_dir / \"review2sent_ids.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                self.rev2ids[obj[\"review_hash\"]] = obj[\"sent_ids\"]\n",
    "\n",
    "        # rows\n",
    "        self.rows = []\n",
    "        with open(self.cache_dir / \"splits\" / split / \"rows.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.rows.append(json.loads(line))\n",
    "\n",
    "        # groupings\n",
    "        with open(self.cache_dir / \"splits\" / split / \"user_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.user_map = {int(k): v for k, v in json.load(f).items()}\n",
    "        with open(self.cache_dir / \"splits\" / split / \"item_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.item_map = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "    def _review_tensor(self, review_hashes):\n",
    "        \"\"\"\n",
    "        Build (rc, S, H) from a list of review hashes.\n",
    "        Takes the first rc; pads with zeros if fewer.\n",
    "        \"\"\"\n",
    "        chosen = (review_hashes[:self.rc] +\n",
    "                  [\"<PAD>\"] * max(0, self.rc - len(review_hashes)))\n",
    "        out = np.zeros((self.rc, self.S, self.H), dtype=np.float32)\n",
    "        for i, rh in enumerate(chosen):\n",
    "            if rh == \"<PAD>\":\n",
    "                continue\n",
    "            ids = self.rev2ids.get(rh, [-1]*self.S)\n",
    "            ids = ids[:self.S] if len(ids) >= self.S else ids + [-1]*(self.S-len(ids))\n",
    "            valid_mask = np.array(ids) >= 0\n",
    "            if valid_mask.any():\n",
    "                out[i, valid_mask, :] = self.emb[np.array(ids)[valid_mask]]\n",
    "        return torch.from_numpy(out)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        u, it, rating, rh = row[\"userID\"], row[\"itemID\"], row[\"rating\"], row[\"review_hash\"]\n",
    "        u_tensor = self._review_tensor(self.user_map.get(u, [rh]))\n",
    "        i_tensor = self._review_tensor(self.item_map.get(it, [rh]))\n",
    "        return u_tensor, i_tensor, torch.tensor([rating], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: quick test that cache & dataset load correctly\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cache_dir = OUT_DIR  # from Cell 3\n",
    "train_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\n",
    "dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "batch = next(iter(dl))\n",
    "u, i, r = batch\n",
    "print(\"User batch:\", tuple(u.shape))  # (B, rc, S, H)\n",
    "print(\"Item batch:\", tuple(i.shape))\n",
    "print(\"Ratings  :\", tuple(r.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, in_dim, k):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "        self.V = nn.Parameter(torch.randn(in_dim, k) * 0.01)\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        xv  = x @ self.V\n",
    "        x2v2 = (x**2) @ (self.V**2)\n",
    "        pairwise = 0.5 * (xv**2 - x2v2).sum(dim=1, keepdim=True)\n",
    "        return linear + pairwise\n",
    "\n",
    "class CNNOverSentences(nn.Module):\n",
    "    def __init__(self, emb_dim, kernel_count=100, kernel_size=3, dropout=0.5, pool=\"max\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(emb_dim, kernel_count, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.act  = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.pool = pool\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B*rc, S, H)\n",
    "        z = self.conv(x.permute(0,2,1))     # (B*rc, K, S)\n",
    "        z = self.act(z).transpose(1,2)      # (B*rc, S, K)\n",
    "        if self.pool == \"max\":\n",
    "            out = z.max(dim=1).values       # (B*rc, K)\n",
    "        else:\n",
    "            out = z.mean(dim=1)\n",
    "        return self.drop(out)\n",
    "\n",
    "\n",
    "class DeepCoNNCached(nn.Module):\n",
    "    def __init__(self, emb_dim, rc=10, kernel_count=100, cnn_out_dim=50):\n",
    "        super().__init__()\n",
    "        self.rc = rc\n",
    "        self.K = kernel_count\n",
    "        self.cnn_u = CNNOverSentences(emb_dim, kernel_count)\n",
    "        self.cnn_i = CNNOverSentences(emb_dim, kernel_count)\n",
    "        self.proj_u = nn.Linear(rc * kernel_count, cnn_out_dim)\n",
    "        self.proj_i = nn.Linear(rc * kernel_count, cnn_out_dim)\n",
    "        self.fm     = FactorizationMachine(cnn_out_dim * 2, 10)\n",
    "    def _encode_side(self, side_tensor, cnn):\n",
    "        B, rc, S, H = side_tensor.shape\n",
    "        flat = side_tensor.reshape(B*rc, S, H)\n",
    "        k   = cnn(flat)                                # (B*rc, K)\n",
    "        k   = k.reshape(B, rc, -1).reshape(B, -1)      # (B, rc*K)\n",
    "        return k\n",
    "    def forward(self, u, i):\n",
    "        u = self._encode_side(u.float(), self.cnn_u)\n",
    "        i = self._encode_side(i.float(), self.cnn_i)\n",
    "        u = self.proj_u(u)\n",
    "        i = self.proj_i(i)\n",
    "        z = torch.cat([u, i], dim=1)\n",
    "        return self.fm(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training loop with progress bars\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "\n",
    "def mse_to_rmse(m): return float(m)**0.5\n",
    "\n",
    "def predict_mse(model, dataloader, device, desc=\"Eval\"):\n",
    "    mse, n = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n",
    "            u,i,r = u.to(device), i.to(device), r.to(device)\n",
    "            preds = model(u,i)\n",
    "            mse += F.mse_loss(preds, r, reduction=\"sum\").item()\n",
    "            n   += r.size(0)\n",
    "    return mse / max(n,1)\n",
    "\n",
    "def predict_mae(model, dataloader, device, desc=\"Eval\"):\n",
    "    mae, n = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n",
    "            u,i,r = u.to(device), i.to(device), r.to(device)\n",
    "            preds = model(u,i)\n",
    "            mae += F.l1_loss(preds, r, reduction=\"sum\").item()\n",
    "            n   += r.size(0)\n",
    "    return mae / max(n,1)\n",
    "\n",
    "def train_loop(train_dl, valid_dl, model, device, epochs=5, lr=2e-3, patience=2, model_path=\"best.pt\"):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    best_loss = float(\"inf\"); bad_epochs = 0\n",
    "    for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        total_loss, total_samples = 0.0, 0\n",
    "        pbar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n",
    "        for u,i,r in pbar:\n",
    "            u,i,r = u.to(device), i.to(device), r.to(device)\n",
    "            preds = model(u,i)\n",
    "            loss  = F.mse_loss(preds, r, reduction=\"sum\")\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total_loss   += loss.item()\n",
    "            total_samples+= r.size(0)\n",
    "            running = total_loss/max(total_samples,1)\n",
    "            pbar.set_postfix(MSE=f\"{running:.4f}\",RMSE=f\"{mse_to_rmse(running):.4f}\")\n",
    "        # validation\n",
    "        valid_mse = predict_mse(model, valid_dl, device, desc=\"Valid\")\n",
    "        print(f\"Epoch {epoch:02d} | Train RMSE {mse_to_rmse(total_loss/max(total_samples,1)):.4f} \"\n",
    "              f\"| Valid RMSE {mse_to_rmse(valid_mse):.4f}\")\n",
    "        if valid_mse < best_loss:\n",
    "            best_loss = valid_mse\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "    print(f\"Best valid RMSE: {mse_to_rmse(best_loss):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: train and test with cached dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cache_dir = OUT_DIR  # from earlier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\n",
    "valid_ds = STCachedDataset(cache_dir, \"valid\", rc=10, S=40)\n",
    "test_ds  = STCachedDataset(cache_dir, \"test\",  rc=10, S=40)\n",
    "\n",
    "pin = torch.cuda.is_available()\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  pin_memory=pin, num_workers=0)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n",
    "\n",
    "model = DeepCoNNCached(emb_dim=train_ds.H, rc=10).to(device)\n",
    "best_path = str(Path(cache_dir)/\"best_model.pt\")\n",
    "\n",
    "train_loop(train_dl, valid_dl, model, device, epochs=5, model_path=best_path)\n",
    "\n",
    "# load best and test\n",
    "best = DeepCoNNCached(emb_dim=train_ds.H, rc=10).to(device)\n",
    "best.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = predict_mse(best, train_dl, device, desc=\"Train\")\n",
    "val_mse   = predict_mse(best, valid_dl, device, desc=\"Valid\")\n",
    "val_mae   = predict_mae(best, valid_dl, device, desc=\"Valid\")\n",
    "test_mse  = predict_mse(best, test_dl,  device, desc=\"Test\")\n",
    "test_mae  = predict_mae(best, test_dl,  device, desc=\"Test\")\n",
    "\n",
    "print(f\"Train  RMSE={mse_to_rmse(train_mse):.4f}\")\n",
    "print(f\"Valid  RMSE={mse_to_rmse(val_mse):.4f}, MAE={val_mae:.4f}\")\n",
    "print(f\"Test   RMSE={mse_to_rmse(test_mse):.4f}, MAE={test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Base working directory (choose whatever makes sense on your machine)\n",
    "WORK_DIR = os.path.abspath(\"./working\")\n",
    "\n",
    "# Subdirectories\n",
    "CACHE_DIR = os.path.join(WORK_DIR, \"st_cache\")\n",
    "FIG_DIR   = os.path.join(WORK_DIR, \"fig\")\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(FIG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "LABEL_NAMES = [\"neg\", \"neutral\", \"pos\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def ratings_to_classes(r: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    r: (B, 1) float tensor of ratings in [1,5]\n",
    "    returns: (B,) long tensor with classes: 0=neg,1=neutral,2=pos\n",
    "    \"\"\"\n",
    "    x = r.squeeze(1)\n",
    "    classes = torch.where(\n",
    "        x <= 2.0, torch.tensor(0, device=x.device),\n",
    "        torch.where(x >= 4.0, torch.tensor(2, device=x.device), torch.tensor(1, device=x.device))\n",
    "    )\n",
    "    return classes.long()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Embedding dim (H):\", train_ds.H)  # should be 300 for GloVe 300d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class CNNOverSentences(nn.Module):\n",
    "    def __init__(self, emb_dim, kernel_count=100, kernel_size=3, dropout=0.3, pool=\"max\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(emb_dim, kernel_count, kernel_size, padding=(kernel_size-1)//2)\n",
    "        self.act  = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.pool = pool\n",
    "    def forward(self, x):\n",
    "        # x: (B*rc, S, H)\n",
    "        z = self.conv(x.permute(0,2,1))     # (B*rc, K, S)\n",
    "        z = self.act(z).transpose(1,2)      # (B*rc, S, K)\n",
    "        out = z.max(dim=1).values if self.pool == \"max\" else z.mean(dim=1)\n",
    "        return self.drop(out)\n",
    "\n",
    "class DeepCoNNCachedClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, rc=10, kernel_count=100, cnn_out_dim=50, num_classes=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.rc = rc\n",
    "        self.cnn_u = CNNOverSentences(emb_dim, kernel_count, dropout=dropout)\n",
    "        self.cnn_i = CNNOverSentences(emb_dim, kernel_count, dropout=dropout)\n",
    "        self.proj_u = nn.Linear(rc * kernel_count, cnn_out_dim)\n",
    "        self.proj_i = nn.Linear(rc * kernel_count, cnn_out_dim)\n",
    "        self.drop   = nn.Dropout(dropout)\n",
    "        # tiny MLP head (a bit more capacity than a single Linear)\n",
    "        self.head   = nn.Sequential(\n",
    "            nn.Linear(cnn_out_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def _encode_side(self, side_tensor, cnn):\n",
    "        B, rc, S, H = side_tensor.shape\n",
    "        flat = side_tensor.reshape(B*rc, S, H)\n",
    "        k   = cnn(flat)                           # (B*rc, K)\n",
    "        k   = k.reshape(B, rc, -1).reshape(B, -1) # (B, rc*K)\n",
    "        return k\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        u = self._encode_side(u.float(), self.cnn_u)\n",
    "        i = self._encode_side(i.float(), self.cnn_i)\n",
    "        u = self.proj_u(u)\n",
    "        i = self.proj_i(i)\n",
    "        z = torch.cat([u, i], dim=1)\n",
    "        z = self.drop(z)\n",
    "        return self.head(z)  # logits (B,3)\n",
    "\n",
    "def compute_class_weights(train_dl, device, num_classes=3):\n",
    "    counts = torch.zeros(num_classes, dtype=torch.float64)\n",
    "    for u,i,r in tqdm(train_dl, desc=\"Class weight scan\", leave=False):\n",
    "        y = ratings_to_classes(r).cpu()\n",
    "        counts += torch.bincount(y, minlength=num_classes).to(torch.float64)\n",
    "    # inverse-frequency weights\n",
    "    weights = counts.sum() / (counts + 1e-8)\n",
    "    return weights.to(device).to(torch.float32)\n",
    "\n",
    "def evaluate_acc(model, dl, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for u,i,r in dl:\n",
    "            logits = model(u.to(device), i.to(device))\n",
    "            y_pred.append(logits.argmax(dim=1).cpu())\n",
    "            y_true.append(ratings_to_classes(r).cpu())\n",
    "    y_true = torch.cat(y_true).numpy()\n",
    "    y_pred = torch.cat(y_pred).numpy()\n",
    "    return float(accuracy_score(y_true, y_pred))\n",
    "\n",
    "# ----- train\n",
    "emb_dim = train_ds.H\n",
    "model = DeepCoNNCachedClassifier(emb_dim=emb_dim, rc=10, kernel_count=100, cnn_out_dim=50, num_classes=3, dropout=0.3).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-6)\n",
    "w = compute_class_weights(train_dl, device)\n",
    "print(\"Class weights:\", w.detach().cpu().numpy().round(3))\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "\n",
    "best_acc, bad_epochs, patience = -1.0, 0, 3\n",
    "best_path = os.path.join(CACHE_DIR, \"best_cls.pt\")\n",
    "\n",
    "for ep in trange(5, desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    running, nb = 0.0, 0\n",
    "    for u,i,r in tqdm(train_dl, desc=f\"Train {ep}\", leave=False):\n",
    "        u,i = u.to(device), i.to(device)\n",
    "        y   = ratings_to_classes(r).to(device)\n",
    "        logits = model(u,i)\n",
    "        loss = criterion(logits, y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running += float(loss.item()); nb += 1\n",
    "\n",
    "    val_acc = evaluate_acc(model, valid_dl, device)\n",
    "    print(f\"Epoch {ep:02d} | Train loss {running/max(nb,1):.4f} | Valid Acc {val_acc:.4f}\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        bad_epochs = 0\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "        if bad_epochs >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "print(f\"Best valid accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# load best for testing\n",
    "best = DeepCoNNCachedClassifier(emb_dim=emb_dim, rc=10, kernel_count=100, cnn_out_dim=50, num_classes=3, dropout=0.0).to(device)\n",
    "best.load_state_dict(torch.load(best_path, map_location=device))\n",
    "best.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Collect predictions\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for u,i,r in test_dl:\n",
    "        logits = best(u.to(device), i.to(device))\n",
    "        y_pred.append(logits.argmax(dim=1).cpu())\n",
    "        y_true.append(ratings_to_classes(r).cpu())\n",
    "\n",
    "y_true = torch.cat(y_true).numpy()\n",
    "y_pred = torch.cat(y_pred).numpy()\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "cm  = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "rep = classification_report(y_true, y_pred, target_names=LABEL_NAMES, digits=4)\n",
    "\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "print(rep)\n",
    "\n",
    "# Save report to file (Kaggle will keep it in /kaggle/working/fig)\n",
    "rep_path = os.path.join(FIG_DIR, \"sentiment_classification_report.txt\")\n",
    "with open(rep_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Accuracy: {acc:.4f}\\n\\n{rep}\\n\")\n",
    "print(\"Classification report saved →\", rep_path)\n",
    "\n",
    "# Plot & save confusion matrix (single-axes, default matplotlib colours)\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5.0))\n",
    "im = ax.imshow(cm)\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_xticks(np.arange(len(LABEL_NAMES)))\n",
    "ax.set_yticks(np.arange(len(LABEL_NAMES)))\n",
    "ax.set_xticklabels(LABEL_NAMES)\n",
    "ax.set_yticklabels(LABEL_NAMES)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "fig.tight_layout()\n",
    "cm_path = os.path.join(FIG_DIR, \"sentiment_confusion_matrix.png\")\n",
    "fig.savefig(cm_path, dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"Confusion matrix saved →\", cm_path)\n",
    "\n",
    "# Display inside Kaggle output pane\n",
    "display(Image(filename=cm_path))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 715814,
     "sourceId": 1246668,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8069290,
     "sourceId": 12764555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8421750,
     "sourceId": 13288272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8672903,
     "sourceId": 13643607,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
