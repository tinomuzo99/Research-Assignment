{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "import os, sys, time, math, json, inspect, argparse, random, re\n",
    "from pathlib import Path\n",
    "\n",
    "# PyData / ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenization (no extra downloads required)\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Hugging Face datasets\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Please install 'datasets' first, e.g. `pip install datasets`\"\n",
    "    ) from e\n",
    "\n",
    "# ---- Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IN_KAGGLE = Path(\"/kaggle\").exists()\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    INPUT_ROOT = Path(\"/kaggle/input\")\n",
    "    WORK_ROOT  = Path(\"/kaggle/working\")\n",
    "\n",
    "    STOPWORDS = INPUT_ROOT / \"digital-music-5\" / \"stopwords.txt\"\n",
    "    PUNCTS    = INPUT_ROOT / \"digital-music-5\" / \"punctuations.txt\"\n",
    "    GLOVE     = INPUT_ROOT / \"glove6b100dtxt\" / \"glove.6B.100d.txt\"\n",
    "\n",
    "    RAW_ALL_BEAUTY = INPUT_ROOT / \"all-beauty\" / \"All_Beauty.jsonl\"\n",
    "    LOCAL_ST_MODEL = INPUT_ROOT / \"minilm-l6-v2-local\" / \"all-MiniLM-L6-v2\"\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "    # if the folders are directly under the project root:\n",
    "    RAW_ALL_BEAUTY = PROJECT_ROOT / \"All_Beauty\" / \"All_Beauty.jsonl\"\n",
    "    GLOVE          = PROJECT_ROOT / \"glove.6B.100d.txt\" / \"glove.6B.100d.txt\"\n",
    "    STOPWORDS      = PROJECT_ROOT / \"Digital_Music_5\" / \"stopwords.txt\"\n",
    "    PUNCTS         = PROJECT_ROOT / \"Digital_Music_5\" / \"punctuations.txt\"\n",
    "    # pointing to the directory that contains config.json, tokenizer.json, etc.\n",
    "    LOCAL_ST_MODEL = PROJECT_ROOT / \"all-MiniLM-L6-v2\" / \"all-MiniLM-L6-v2\"\n",
    "\n",
    "    ARTIFACTS = PROJECT_ROOT / \"artifacts\"\n",
    "\n",
    "# Output directories as Path\n",
    "WORK_DIR      = WORK_ROOT if IN_KAGGLE else ARTIFACTS\n",
    "SAVE_DATA_DIR = WORK_DIR / \"Amazon_Fashion\"\n",
    "MODEL_DIR     = WORK_DIR / \"model\"\n",
    "FIG_DIR       = WORK_DIR / \"fig\"\n",
    "ST_CACHE_DIR  = WORK_DIR / \"st_cache\"\n",
    "\n",
    "# Create directories\n",
    "for d in [SAVE_DATA_DIR, MODEL_DIR, FIG_DIR, ST_CACHE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [RAW_ALL_BEAUTY, GLOVE, STOPWORDS, PUNCTS, LOCAL_ST_MODEL]:\n",
    "    print(p, \"exists:\", p.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset  # optional fallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "def load_all_beauty_local(jsonl_path: str | Path | None = None):\n",
    "    \"\"\"\n",
    "    Load All_Beauty.jsonl (Kaggle or local) and return a pandas DataFrame\n",
    "    with standardized columns: userID, itemID, review, rating.\n",
    "    \"\"\"\n",
    "    if jsonl_path is None:\n",
    "        jsonl_path = RAW_ALL_BEAUTY\n",
    "    jsonl_path = Path(jsonl_path)\n",
    "\n",
    "    needed = (\"user_id\", \"asin\", \"text\", \"rating\")\n",
    "\n",
    "    def _standardize_cols(df):\n",
    "        alt_map = {\n",
    "            \"reviewText\": \"text\",\n",
    "            \"overall\": \"rating\",\n",
    "            \"user\": \"user_id\",\n",
    "            \"item\": \"asin\",\n",
    "        }\n",
    "        for old, new in alt_map.items():\n",
    "            if old in df.columns and new not in df.columns:\n",
    "                df[new] = df[old]\n",
    "\n",
    "        missing = [c for c in needed if c not in df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"Missing required columns {missing}. \"\n",
    "                \"Make sure your JSONL has keys like: user_id, asin, text, rating.\"\n",
    "            )\n",
    "\n",
    "        df = df[list(needed)].copy()\n",
    "        df.columns = [\"userID\", \"itemID\", \"review\", \"rating\"]\n",
    "        df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n",
    "        df = df[df[\"rating\"].notnull()]\n",
    "        df = df[df[\"review\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        df = pd.read_json(jsonl_path, lines=True)\n",
    "        return _standardize_cols(df)\n",
    "    except Exception as e_pd:\n",
    "        try:\n",
    "            ds = load_dataset(\"json\", data_files=str(jsonl_path), split=\"train\")\n",
    "            df = ds.to_pandas()\n",
    "            return _standardize_cols(df)\n",
    "        except Exception as e_hf:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load JSONL via pandas ({type(e_pd).__name__}: {e_pd}) \"\n",
    "                f\"and datasets ({type(e_hf).__name__}: {e_hf}).\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- keep your helpers as-is ---\n",
    "def _read_list(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Required file not found: {path}\\n\"\n",
    "            \"Place a plain-text file with one token per line.\"\n",
    "        )\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return set(ln.strip() for ln in f if ln.strip())\n",
    "\n",
    "def process_df_to_csv(df, stopwords_path, puncts_path, train_rate, csv_path):\n",
    "    # Map IDs to contiguous integers\n",
    "    df[\"userID\"] = df[\"userID\"].astype(\"category\").cat.codes\n",
    "    df[\"itemID\"] = df[\"itemID\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # Load stopwords/punctuations\n",
    "    stop_words   = _read_list(stopwords_path)\n",
    "    punctuations = _read_list(puncts_path)\n",
    "    tok = WordPunctTokenizer()\n",
    "\n",
    "    def clean_review(review: str) -> str:\n",
    "        rv = review.lower()\n",
    "        for p in punctuations:\n",
    "            rv = rv.replace(p, \" \")\n",
    "        toks = tok.tokenize(rv)\n",
    "        toks = [w for w in toks if w not in stop_words]\n",
    "        return \" \".join(toks)\n",
    "\n",
    "    print(\"#### Cleaning text (this can take a while on large splits)...\")\n",
    "    df[\"review\"] = df[\"review\"].apply(clean_review)\n",
    "\n",
    "    # Train/valid/test split\n",
    "    train_df, valid_test_df = train_test_split(df, test_size=1 - train_rate, random_state=3)\n",
    "    valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=4)\n",
    "\n",
    "    os.makedirs(csv_path, exist_ok=True)\n",
    "    train_df.to_csv(os.path.join(csv_path, \"train.csv\"), index=False, header=False)\n",
    "    valid_df.to_csv(os.path.join(csv_path, \"valid.csv\"), index=False, header=False)\n",
    "    test_df .to_csv(os.path.join(csv_path, \"test.csv\"),  index=False, header=False)\n",
    "\n",
    "    print(f\"#### Saved CSVs to {csv_path}\")\n",
    "    print(f\"#### Split sizes: train {len(train_df)}, valid {len(valid_df)}, test {len(test_df)}\")\n",
    "    print(f\"#### Totals: {len(df)} reviews, {df['userID'].nunique()} users, {df['itemID'].nunique()} items.\")\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train = SAVE_DATA_DIR / \"train.csv\"\n",
    "\n",
    "if not csv_train.exists():\n",
    "    df_raw = load_all_beauty_local()  # uses Kaggle or local automatically\n",
    "    _ = process_df_to_csv(\n",
    "        df_raw,\n",
    "        stopwords_path=STOPWORDS,\n",
    "        puncts_path=PUNCTS,\n",
    "        train_rate=0.8,\n",
    "        csv_path=SAVE_DATA_DIR,\n",
    "    )\n",
    "else:\n",
    "    print(\"CSV files already exist — skipping reprocessing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now(f='%Y-%m-%d %H:%M:%S'):\n",
    "    return time.strftime(f, time.localtime())\n",
    "\n",
    "class Config:\n",
    "    # Device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Training\n",
    "    train_epochs        = 5\n",
    "    batch_size          = 128\n",
    "    learning_rate       = 2e-3\n",
    "    l2_regularization   = 1e-6\n",
    "    learning_rate_decay = 0.99\n",
    "    patience            = 3\n",
    "\n",
    "    # Files\n",
    "    word2vec_file = GLOVE\n",
    "    train_file    = os.path.join(SAVE_DATA_DIR, 'train.csv')\n",
    "    valid_file    = os.path.join(SAVE_DATA_DIR, 'valid.csv')\n",
    "    test_file     = os.path.join(SAVE_DATA_DIR, 'test.csv')\n",
    "    model_file    = os.path.join(MODEL_DIR, 'best_model.pt')\n",
    "\n",
    "    # Data shaping\n",
    "    review_count         = 10    # number of reviews per side\n",
    "    review_length        = 40    # tokens per review\n",
    "    lowest_review_count  = 2\n",
    "    PAD_WORD             = '<UNK>'\n",
    "\n",
    "    # Model sizes\n",
    "    kernel_count = 100\n",
    "    kernel_size  = 3\n",
    "    dropout_prob = 0.5\n",
    "    cnn_out_dim  = 50\n",
    "\n",
    "    def __init__(self):\n",
    "        # Allow CLI/nb override (no-op by default)\n",
    "        attributes = inspect.getmembers(self, lambda a: not inspect.isfunction(a))\n",
    "        attributes = list(filter(lambda x: not x[0].startswith('__'), attributes))\n",
    "        parser = argparse.ArgumentParser(add_help=False)\n",
    "        for key, val in attributes:\n",
    "            parser.add_argument('--' + key, dest=key, type=type(val), default=val)\n",
    "        args, _ = parser.parse_known_args([])\n",
    "        for key, val in args.__dict__.items():\n",
    "            setattr(self, key, val)\n",
    "\n",
    "    def __str__(self):\n",
    "        attributes = inspect.getmembers(self, lambda a: not inspect.isfunction(a))\n",
    "        attributes = list(filter(lambda x: not x[0].startswith('__'), attributes))\n",
    "        return \"\\n\".join([f\"{k} = {v}\" for k, v in attributes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & small utils\n",
    "import os, json, re, csv, hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def simple_sentence_split(text: str, max_sentences: int):\n",
    "    \"\"\"Split on . ! ? and keep up to max_sentences.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sents = [s.strip() for s in parts if s.strip()]\n",
    "    return sents[:max_sentences]\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split(csv_path: str):\n",
    "    \"\"\"Reads CSV with no header: userID,itemID,review,rating\"\"\"\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.reader(f)\n",
    "        for user_id, item_id, review, rating in rdr:\n",
    "            rows.append((int(user_id), int(item_id), review, float(rating)))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(word2vec_file):\n",
    "    \"\"\"\n",
    "    Loads GloVe text file into:\n",
    "    - word_emb: np.ndarray shape (V, D), row 0 is <UNK> = 0-vector\n",
    "    - word_dict: {token -> idx}\n",
    "    \"\"\"\n",
    "    if not os.path.exists(word2vec_file):\n",
    "        raise FileNotFoundError(\n",
    "            f\"GloVe file not found at {word2vec_file}.\\n\"\n",
    "            \"Place 'glove.6B.100d.txt' under assets/ or Kaggle input.\"\n",
    "        )\n",
    "    word_emb, word_dict = [], {}\n",
    "    word_emb.append([0.0])           # temp row 0; fix dim after first vec\n",
    "    word_dict['<UNK>'] = 0\n",
    "    with open(word2vec_file, encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            toks = line.split(' ')\n",
    "            word, vec = toks[0], toks[1:]\n",
    "            if not vec:\n",
    "                continue\n",
    "            vec = [float(x) for x in vec]\n",
    "            if i == 0:\n",
    "                word_emb[0] = [0.0] * len(vec)  # set UNK dim\n",
    "            word_emb.append(vec)\n",
    "            word_dict[word] = len(word_dict)\n",
    "    return np.array(word_emb, dtype=np.float32), word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, csv, re, hashlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def simple_sentence_split(text: str, max_sentences: int):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sents = [s.strip() for s in parts if s.strip()]\n",
    "    return sents[:max_sentences]\n",
    "\n",
    "def read_split(csv_path: str):\n",
    "    rows = []\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        rdr = csv.reader(f)\n",
    "        for user_id, item_id, review, rating in rdr:\n",
    "            rows.append((int(user_id), int(item_id), review, float(rating)))\n",
    "    return rows\n",
    "\n",
    "def precompute_sent_embeddings_glove(\n",
    "    train_csv: str,\n",
    "    valid_csv: str,\n",
    "    test_csv: str,\n",
    "    out_dir: str,\n",
    "    word2vec_file: str,\n",
    "    S: int = 40,\n",
    "    lowercase: bool = True,\n",
    "    batch_write: int = 5000,  # how many sentences to aggregate before writing (memmap is fine, but this keeps RAM low)\n",
    "):\n",
    "    \"\"\"\n",
    "    GloVe version of precompute:\n",
    "      - Build unique sentence inventory (cap S per review)\n",
    "      - Embed each sentence by mean of GloVe word vectors (UNK=0 ignored if no known tokens)\n",
    "      - Write the same cache layout your STCachedDataset expects\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    ensure_dir(out_dir)\n",
    "    splits_dir = out_dir / \"splits\"\n",
    "    ensure_dir(splits_dir)\n",
    "\n",
    "    # 1) Read all splits\n",
    "    split_paths = {\"train\": Path(train_csv), \"valid\": Path(valid_csv), \"test\": Path(test_csv)}\n",
    "    splits = {k: read_split(str(v)) for k, v in split_paths.items()}\n",
    "\n",
    "    # 2) Collect unique reviews + per-split manifests\n",
    "    all_reviews = {}          # review_hash -> raw_review_text\n",
    "    split_rows = {}           # split -> list[dict]\n",
    "    user_reviews = {}         # split -> {userID: [review_hash,...]}\n",
    "    item_reviews = {}         # split -> {itemID: [review_hash,...]}\n",
    "\n",
    "    for split, rows in splits.items():\n",
    "        s_rows = []\n",
    "        u_map = {}\n",
    "        i_map = {}\n",
    "        for (u, it, review, rating) in rows:\n",
    "            rtxt = review.lower() if lowercase else review\n",
    "            r_hash = sha1(rtxt)\n",
    "            if r_hash not in all_reviews:\n",
    "                all_reviews[r_hash] = rtxt\n",
    "            s_rows.append({\"userID\": u, \"itemID\": it, \"rating\": rating, \"review_hash\": r_hash})\n",
    "            u_map.setdefault(u, []).append(r_hash)\n",
    "            i_map.setdefault(it, []).append(r_hash)\n",
    "        split_rows[split] = s_rows\n",
    "        user_reviews[split] = {str(k): v for k, v in u_map.items()}\n",
    "        item_reviews[split] = {str(k): v for k, v in i_map.items()}\n",
    "\n",
    "    # 3) Build deduped sentence inventory\n",
    "    sentence_to_id = {}\n",
    "    sentences = []  # index -> text\n",
    "    review_to_sentids = {}  # review_hash -> [S] sentence ids (pad with -1)\n",
    "\n",
    "    for r_hash, rtxt in all_reviews.items():\n",
    "        sents = simple_sentence_split(rtxt, S)\n",
    "        ids = []\n",
    "        for s in sents:\n",
    "            if s not in sentence_to_id:\n",
    "                sentence_to_id[s] = len(sentences)\n",
    "                sentences.append(s)\n",
    "            ids.append(sentence_to_id[s])\n",
    "        ids = (ids + [-1]*(S - len(ids))) if len(ids) < S else ids[:S]\n",
    "        review_to_sentids[r_hash] = ids\n",
    "\n",
    "    print(f\"[build] Unique reviews: {len(all_reviews):,}\")\n",
    "    print(f\"[build] Unique sentences: {len(sentences):,}\")\n",
    "\n",
    "    # 4) Load GloVe\n",
    "    word_emb, word_dict = load_embedding(word2vec_file)\n",
    "    D = int(word_emb.shape[1])\n",
    "    print(f\"[glove] Vocab: {word_emb.shape[0]:,}, Dim: {D}\")\n",
    "\n",
    "    # 5) Encode all unique sentences (mean of token vectors)\n",
    "    tok = WordPunctTokenizer()\n",
    "    N = len(sentences)\n",
    "    EMB = np.memmap(out_dir / \"embeddings.npy\", dtype=\"float32\", mode=\"w+\", shape=(N, D))\n",
    "\n",
    "    def sent_vec(text: str):\n",
    "        tokens = tok.tokenize(text.lower())\n",
    "        idxs = [word_dict.get(w, 0) for w in tokens]  # 0 is <UNK>\n",
    "        if not idxs:\n",
    "            return np.zeros((D,), dtype=np.float32)\n",
    "        # exclude UNK rows to avoid biasing means when most words are unseen\n",
    "        real = [i for i in idxs if i != 0]\n",
    "        if not real:\n",
    "            return np.zeros((D,), dtype=np.float32)\n",
    "        vecs = word_emb[np.array(real)]\n",
    "        return vecs.mean(axis=0).astype(np.float32)\n",
    "\n",
    "    # Write in chunks (optional; memmap can handle one-by-one too)\n",
    "    buf = []\n",
    "    starts = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        buf.append(sent_vec(s))\n",
    "        if len(buf) >= batch_write:\n",
    "            start = i + 1 - len(buf)\n",
    "            EMB[start:start+len(buf), :] = np.stack(buf, axis=0)\n",
    "            buf.clear()\n",
    "    if buf:\n",
    "        start = N - len(buf)\n",
    "        EMB[start: start+len(buf), :] = np.stack(buf, axis=0)\n",
    "        buf.clear()\n",
    "    del EMB\n",
    "    print(\"[encode] embeddings.npy written.\")\n",
    "\n",
    "    # 6) Metadata files\n",
    "    with open(out_dir / \"sentences.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sid, txt in enumerate(sentences):\n",
    "            f.write(json.dumps({\"sent_id\": sid, \"text\": txt}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    with open(out_dir / \"review2sent_ids.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for r_hash, ids in review_to_sentids.items():\n",
    "            f.write(json.dumps({\"review_hash\": r_hash, \"sent_ids\": ids}) + \"\\n\")\n",
    "\n",
    "    # Per-split artifacts\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        sp = splits_dir / split\n",
    "        ensure_dir(sp)\n",
    "        with open(sp / \"rows.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in split_rows[split]:\n",
    "                f.write(json.dumps(row) + \"\\n\")\n",
    "        with open(sp / \"user_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(user_reviews[split], f)\n",
    "        with open(sp / \"item_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(item_reviews[split], f)\n",
    "\n",
    "    print(f\"[done] Wrote cache to: {out_dir.resolve()}\")\n",
    "    print(\"      Files:\")\n",
    "    print(\"       - embeddings.npy  (shape: \", (N, D), \")\")\n",
    "    print(\"       - sentences.jsonl\")\n",
    "    print(\"       - review2sent_ids.jsonl\")\n",
    "    print(\"       - splits/*/rows.jsonl, user_reviews.json, item_reviews.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: run precompute — EDIT THESE PATHS\n",
    "TRAIN_CSV = SAVE_DATA_DIR / \"train.csv\"\n",
    "VALID_CSV = SAVE_DATA_DIR / \"valid.csv\"\n",
    "TEST_CSV  = SAVE_DATA_DIR / \"test.csv\"\n",
    "OUT_DIR   = ST_CACHE_DIR\n",
    "\n",
    "precompute_sent_embeddings_glove(\n",
    "    train_csv=TRAIN_CSV,\n",
    "    valid_csv=VALID_CSV,\n",
    "    test_csv=TEST_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    "    word2vec_file=GLOVE,   # e.g. \".../glove.6B.300d.txt\"\n",
    "    S=40,\n",
    "    lowercase=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: STCachedDataset (fixed rc/S) for fast numeric I/O training\n",
    "import json, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class STCachedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads numeric cache produced by precompute_sent_embeddings (fixed rc/S view).\n",
    "    - embeddings.npy  -> (N_sentences, H) float32 (memmap)\n",
    "    - review2sent_ids.jsonl -> review_hash -> [S] sentence ids (-1 pad)\n",
    "    - splits/{split}/rows.jsonl\n",
    "    - splits/{split}/user_reviews.json, item_reviews.json\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir, split, rc=10, S=40):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.split = split\n",
    "        self.rc = rc\n",
    "        self.S = S\n",
    "\n",
    "        # embeddings\n",
    "        self.emb = np.memmap(self.cache_dir / \"embeddings.npy\", dtype=\"float32\", mode=\"r\")\n",
    "        # discover H\n",
    "        N = self.emb.size\n",
    "        with open(self.cache_dir / \"sentences.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            n_sent = sum(1 for _ in f)\n",
    "        H = N // n_sent\n",
    "        self.emb = self.emb.reshape(n_sent, H)\n",
    "        self.H = H\n",
    "\n",
    "        # review -> sent_ids\n",
    "        self.rev2ids = {}\n",
    "        with open(self.cache_dir / \"review2sent_ids.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                self.rev2ids[obj[\"review_hash\"]] = obj[\"sent_ids\"]\n",
    "\n",
    "        # rows\n",
    "        self.rows = []\n",
    "        with open(self.cache_dir / \"splits\" / split / \"rows.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.rows.append(json.loads(line))\n",
    "\n",
    "        # groupings\n",
    "        with open(self.cache_dir / \"splits\" / split / \"user_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.user_map = {int(k): v for k, v in json.load(f).items()}\n",
    "        with open(self.cache_dir / \"splits\" / split / \"item_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            self.item_map = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "    def _review_tensor(self, review_hashes):\n",
    "        \"\"\"\n",
    "        Build (rc, S, H) from a list of review hashes.\n",
    "        Takes the first rc; pads with zeros if fewer.\n",
    "        \"\"\"\n",
    "        chosen = (review_hashes[:self.rc] +\n",
    "                  [\"<PAD>\"] * max(0, self.rc - len(review_hashes)))\n",
    "        out = np.zeros((self.rc, self.S, self.H), dtype=np.float32)\n",
    "        for i, rh in enumerate(chosen):\n",
    "            if rh == \"<PAD>\":\n",
    "                continue\n",
    "            ids = self.rev2ids.get(rh, [-1]*self.S)\n",
    "            ids = ids[:self.S] if len(ids) >= self.S else ids + [-1]*(self.S-len(ids))\n",
    "            valid_mask = np.array(ids) >= 0\n",
    "            if valid_mask.any():\n",
    "                out[i, valid_mask, :] = self.emb[np.array(ids)[valid_mask]]\n",
    "        return torch.from_numpy(out)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        u, it, rating, rh = row[\"userID\"], row[\"itemID\"], row[\"rating\"], row[\"review_hash\"]\n",
    "        u_tensor = self._review_tensor(self.user_map.get(u, [rh]))\n",
    "        i_tensor = self._review_tensor(self.item_map.get(it, [rh]))\n",
    "        return u_tensor, i_tensor, torch.tensor([rating], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: quick test that cache & dataset load correctly\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cache_dir = OUT_DIR  # from Cell 3\n",
    "train_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\n",
    "dl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "batch = next(iter(dl))\n",
    "u, i, r = batch\n",
    "print(\"User batch:\", tuple(u.shape))  # (B, rc, S, H)\n",
    "print(\"Item batch:\", tuple(i.shape))\n",
    "print(\"Ratings  :\", tuple(r.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FactorizationMachine(nn.Module):\n",
    "    def __init__(self, in_dim, k):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "        self.V = nn.Parameter(torch.randn(in_dim, k) * 0.01)\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        xv  = x @ self.V\n",
    "        x2v2 = (x**2) @ (self.V**2)\n",
    "        pairwise = 0.5 * (xv**2 - x2v2).sum(dim=1, keepdim=True)\n",
    "        return linear + pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, BertConfig, BertModel\n",
    "\n",
    "class HFSeqEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Tries to load a pretrained HF backbone locally (no networking).\n",
    "    If unavailable, falls back to a randomly-initialised tiny BERT.\n",
    "    \"\"\"\n",
    "    def __init__(self, glove_dim: int,\n",
    "                 backbone: str = \"prajjwal1/bert-tiny\",\n",
    "                 dropout: float = 0.1,\n",
    "                 freeze_backbone: bool = False,\n",
    "                 offline_only: bool = True,   # key flag: avoid network calls\n",
    "                 fallback_hidden: int = 128,  # used if we build random tiny BERT\n",
    "                 fallback_layers: int = 2,\n",
    "                 fallback_heads: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encourage transformers to avoid network calls\n",
    "        os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "\n",
    "        loaded = False\n",
    "        try:\n",
    "            # Try to use a locally cached pretrained model (no internet)\n",
    "            self.config = AutoConfig.from_pretrained(backbone, local_files_only=offline_only)\n",
    "            self.backbone = AutoModel.from_pretrained(backbone, local_files_only=offline_only)\n",
    "            loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"[HFSeqEncoder] Could not load '{backbone}' locally: {type(e).__name__}: {e}\")\n",
    "            print(\"[HFSeqEncoder] Falling back to a randomly initialised tiny BERT.\")\n",
    "            # Build a small BERT from scratch (random init) so shapes are sane\n",
    "            self.config = BertConfig(\n",
    "                hidden_size=fallback_hidden,\n",
    "                num_hidden_layers=fallback_layers,\n",
    "                num_attention_heads=fallback_heads,\n",
    "                intermediate_size=fallback_hidden * 4,\n",
    "                max_position_embeddings=512,\n",
    "                vocab_size=30522,\n",
    "                hidden_dropout_prob=dropout,\n",
    "                attention_probs_dropout_prob=dropout,\n",
    "            )\n",
    "            self.backbone = BertModel(self.config)\n",
    "\n",
    "        if freeze_backbone and loaded:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.in_proj = nn.Linear(glove_dim, self.config.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):  # x: (B*rc, S, H_glove)\n",
    "        # Build attention mask: zero-rows are padding\n",
    "        attn_mask = (x.abs().sum(dim=-1) > 0)  # bool mask (B*rc, S)\n",
    "\n",
    "        x = self.in_proj(x)  # (B*rc, S, hidden)\n",
    "        out = self.backbone(inputs_embeds=x, attention_mask=attn_mask).last_hidden_state  # (B*rc, S, hidden)\n",
    "\n",
    "        # masked mean over S\n",
    "        mask = attn_mask.unsqueeze(-1)                        # (B*rc, S, 1)\n",
    "        summed = (out * mask).sum(dim=1)                      # (B*rc, hidden)\n",
    "        denom = mask.sum(dim=1).clamp(min=1).float()          # (B*rc, 1)\n",
    "        pooled = summed / denom\n",
    "        return self.dropout(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Drop this cell before the training loop -----\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepCoNNTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    GloVe -> projection -> (tiny) BERT encoder per review-sentence block,\n",
    "    pooled across sentences and across rc reviews, then MLP to a scalar.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        glove_dim: int,          # must equal train_ds.H (e.g., 100 for glove.6B.100d)\n",
    "        rc: int = 10,            # number of reviews per side (matches STCachedDataset)\n",
    "        backbone: str = \"prajjwal1/bert-tiny\",\n",
    "        proj_dim: int = 128,     # hidden size to use if we build our own tiny BERT\n",
    "        dropout: float = 0.2,\n",
    "        freeze_backbone: bool = False,\n",
    "        offline_only: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rc = rc\n",
    "\n",
    "        # Two identical encoders (weights NOT shared by default in DeepCoNN literature).\n",
    "        # If you want to share, reuse the same module instance for both.\n",
    "        self.user_enc = HFSeqEncoder(\n",
    "            glove_dim=glove_dim,\n",
    "            backbone=backbone,\n",
    "            dropout=dropout,\n",
    "            freeze_backbone=freeze_backbone,\n",
    "            offline_only=offline_only,\n",
    "            fallback_hidden=proj_dim,\n",
    "        )\n",
    "        self.item_enc = HFSeqEncoder(\n",
    "            glove_dim=glove_dim,\n",
    "            backbone=backbone,\n",
    "            dropout=dropout,\n",
    "            freeze_backbone=freeze_backbone,\n",
    "            offline_only=offline_only,\n",
    "            fallback_hidden=proj_dim,\n",
    "        )\n",
    "\n",
    "        hidden = self.user_enc.config.hidden_size  # from the backbone (or fallback config)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def _encode_stack(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, rc, S, H_glove)\n",
    "        returns pooled: (B, hidden) by encoding each review then averaging over rc\n",
    "        \"\"\"\n",
    "        B, rc, S, H = x.shape\n",
    "        x = x.view(B * rc, S, H)              # (B*rc, S, H)\n",
    "        rep = self.user_enc(x) if self._route == \"user\" else self.item_enc(x)  # (B*rc, hidden)\n",
    "        rep = rep.view(B, rc, -1).mean(dim=1) # (B, hidden), mean over rc\n",
    "        return rep\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        \"\"\"\n",
    "        u, i: (B, rc, S, H_glove) float tensors from STCachedDataset\n",
    "        returns: (B, 1) predicted ratings\n",
    "        \"\"\"\n",
    "        # encode users\n",
    "        self._route = \"user\"\n",
    "        u_rep = self._encode_stack(u)\n",
    "        # encode items\n",
    "        self._route = \"item\"\n",
    "        i_rep = self._encode_stack(i)\n",
    "        # concat and predict\n",
    "        x = torch.cat([u_rep, i_rep], dim=-1)  # (B, 2*hidden)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training loop with progress bars\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "\n",
    "def mse_to_rmse(m): return float(m)**0.5\n",
    "\n",
    "def predict_mse(model, dataloader, device, desc=\"Eval\"):\n",
    "    mse, n = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n",
    "            u,i,r = u.to(device), i.to(device), r.to(device)\n",
    "            preds = model(u,i)\n",
    "            mse += F.mse_loss(preds, r, reduction=\"sum\").item()\n",
    "            n   += r.size(0)\n",
    "    return mse / max(n,1)\n",
    "\n",
    "def predict_mae(model, dataloader, device, desc=\"Eval\"):\n",
    "    mae, n = 0.0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n",
    "            u,i,r = u.to(device), i.to(device), r.to(device)\n",
    "            preds = model(u,i)\n",
    "            mae += F.l1_loss(preds, r, reduction=\"sum\").item()\n",
    "            n   += r.size(0)\n",
    "    return mae / max(n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train_dl, valid_dl, model, device, epochs=5, lr=2e-3,\n",
    "               patience=2, model_path=\"best.pt\"):\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    best_mse = float(\"inf\")\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        total_loss, total_samples = 0.0, 0\n",
    "        pbar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n",
    "\n",
    "        for u, i, r in pbar:\n",
    "            u, i, r = u.to(device), i.to(device), r.to(device)\n",
    "            preds = model(u, i)\n",
    "            loss = F.mse_loss(preds, r, reduction=\"sum\")\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_samples += r.size(0)\n",
    "            running_mse = total_loss / max(total_samples, 1)\n",
    "            pbar.set_postfix(MSE=f\"{running_mse:.4f}\",\n",
    "                             RMSE=f\"{running_mse**0.5:.4f}\")\n",
    "\n",
    "        # ---- Validation phase ----\n",
    "        valid_mse = predict_mse(model, valid_dl, device, desc=\"Valid\")\n",
    "        valid_mae = predict_mae(model, valid_dl, device, desc=\"Valid\")\n",
    "\n",
    "        train_mse = total_loss / max(total_samples, 1)\n",
    "        train_rmse = train_mse ** 0.5\n",
    "        valid_rmse = valid_mse ** 0.5\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train RMSE {train_rmse:.4f} | \"\n",
    "            f\"Valid RMSE {valid_rmse:.4f} | \"\n",
    "            f\"Valid MAE {valid_mae:.4f}\"\n",
    "        )\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if valid_mse < best_mse:\n",
    "            best_mse = valid_mse\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Best valid RMSE: {best_mse**0.5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Replace your model init + training block with this =====\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cache_dir = OUT_DIR\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\n",
    "valid_ds = STCachedDataset(cache_dir, \"valid\", rc=10, S=40)\n",
    "test_ds  = STCachedDataset(cache_dir, \"test\",  rc=10, S=40)\n",
    "\n",
    "pin = torch.cuda.is_available()\n",
    "g = torch.Generator(); g.manual_seed(42)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  pin_memory=pin, num_workers=0)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n",
    "\n",
    "# IMPORTANT: glove_dim must match your cached sentence-embedding dim (= train_ds.H)\n",
    "model = DeepCoNNTransformer(\n",
    "    glove_dim=train_ds.H,\n",
    "    rc=10,\n",
    "    backbone=\"prajjwal1/bert-tiny\",   # or 'sentence-transformers/all-MiniLM-L6-v2' if cached\n",
    "    proj_dim=128,\n",
    "    dropout=0.2,\n",
    "    freeze_backbone=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_path = str(Path(cache_dir)/\"best_model.pt\")\n",
    "train_loop(train_dl, valid_dl, model, device, epochs=5, model_path=best_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = DeepCoNNTransformer(glove_dim=train_ds.H, rc=10, backbone=\"prajjwal1/bert-tiny\").to(device)\n",
    "best.load_state_dict(torch.load(best_path, map_location=device))\n",
    "mse = predict_mse(best, test_dl, device, desc=\"Test\")\n",
    "mae = predict_mae(best, test_dl, device, desc=\"Test\")\n",
    "print(f\"Test RMSE={mse**0.5:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell: rating -> class bins (0,1,2)\n",
    "def rating_to_class(r):\n",
    "    # 1&2 -> 0 (neg), 3 -> 1 (neu), 4&5 -> 2 (pos)\n",
    "    if r <= 2.0: \n",
    "        return 0\n",
    "    elif r >= 4.0:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell: classification dataset wrapper\n",
    "class STCachedDatasetCls(STCachedDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        u_tensor, i_tensor, rating = super().__getitem__(idx)  # rating: float tensor [1]\n",
    "        y = rating_to_class(float(rating.item()))\n",
    "        return u_tensor, i_tensor, torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell: classifier model\n",
    "class DeepCoNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        glove_dim: int,\n",
    "        rc: int = 10,\n",
    "        backbone: str = \"prajjwal1/bert-tiny\",\n",
    "        proj_dim: int = 128,\n",
    "        dropout: float = 0.2,\n",
    "        freeze_backbone: bool = False,\n",
    "        offline_only: bool = True,\n",
    "        num_classes: int = 3,\n",
    "        share_backbone: bool = False,  # set True if you want to share encoders\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rc = rc\n",
    "\n",
    "        if share_backbone:\n",
    "            shared = HFSeqEncoder(glove_dim, backbone, dropout, freeze_backbone, offline_only, proj_dim)\n",
    "            self.user_enc = shared\n",
    "            self.item_enc = shared\n",
    "        else:\n",
    "            self.user_enc = HFSeqEncoder(glove_dim, backbone, dropout, freeze_backbone, offline_only, proj_dim)\n",
    "            self.item_enc = HFSeqEncoder(glove_dim, backbone, dropout, freeze_backbone, offline_only, proj_dim)\n",
    "\n",
    "        hidden = self.user_enc.config.hidden_size\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    def _encode_stack(self, x, route=\"user\"):\n",
    "        B, rc, S, H = x.shape\n",
    "        x = x.view(B * rc, S, H)              # (B*rc, S, H)\n",
    "        rep = self.user_enc(x) if route == \"user\" else self.item_enc(x)\n",
    "        rep = rep.view(B, rc, -1).mean(dim=1) # (B, hidden), mean over rc\n",
    "        return rep\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        u_rep = self._encode_stack(u, route=\"user\")\n",
    "        i_rep = self._encode_stack(i, route=\"item\")\n",
    "        x = torch.cat([u_rep, i_rep], dim=-1)   # (B, 2*hidden)\n",
    "        logits = self.out(x)                    # (B, 3)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell: train/eval for classification\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def evaluate_cls(model, dataloader, device, desc=\"Eval\"):\n",
    "    model.eval()\n",
    "    all_y, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for u, i, y in tqdm(dataloader, desc=desc, leave=False):\n",
    "            u, i, y = u.to(device), i.to(device), y.to(device)\n",
    "            logits = model(u, i)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_y.append(y.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "    y_true = np.concatenate(all_y) if all_y else np.array([])\n",
    "    y_pred = np.concatenate(all_pred) if all_pred else np.array([])\n",
    "    acc = accuracy_score(y_true, y_pred) if y_true.size else 0.0\n",
    "    return acc, y_true, y_pred\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', fname=None):\n",
    "    \"\"\"\n",
    "    Pure matplotlib (no seaborn). If fname is provided, saves the figure.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), dpi=150)\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(\n",
    "        xticks=np.arange(len(classes)),\n",
    "        yticks=np.arange(len(classes)),\n",
    "        xticklabels=classes, yticklabels=classes,\n",
    "        ylabel='True label',\n",
    "        title=title,\n",
    "        xlabel='Predicted label',\n",
    "    )\n",
    "    # annotate cells\n",
    "    fmtr = \"{:.2f}\" if normalize else \"{:d}\"\n",
    "    thresh = cm.max() / 2. if cm.size else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, fmtr.format(cm[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    if fname:\n",
    "        os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "        plt.savefig(fname, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def train_loop_cls(train_dl, valid_dl, model, device, epochs=5, lr=2e-3,\n",
    "                   patience=2, model_path=\"best_cls.pt\", class_weights=None):\n",
    "    model = model.to(device)\n",
    "    # optional class weighting for imbalance\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device) if class_weights is not None else None)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "    best_acc = -1.0\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        running_loss, seen = 0.0, 0\n",
    "        pbar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n",
    "        for u, i, y in pbar:\n",
    "            u, i, y = u.to(device), i.to(device), y.to(device)\n",
    "            logits = model(u, i)\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "            seen += y.size(0)\n",
    "            pbar.set_postfix(LOSS=f\"{running_loss/max(seen,1):.4f}\")\n",
    "\n",
    "        # Evaluate\n",
    "        train_acc, _, _ = evaluate_cls(model, train_dl, device, desc=\"Train Eval\")\n",
    "        valid_acc, _, _ = evaluate_cls(model, valid_dl, device, desc=\"Valid Eval\")\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | Train Acc {train_acc:.4f} | Valid Acc {valid_acc:.4f}\")\n",
    "\n",
    "        # Early stopping on accuracy\n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Best Valid Accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Cell: data loaders for classification\n",
    "train_ds_c = STCachedDatasetCls(cache_dir, \"train\", rc=10, S=40)\n",
    "valid_ds_c = STCachedDatasetCls(cache_dir, \"valid\", rc=10, S=40)\n",
    "test_ds_c  = STCachedDatasetCls(cache_dir, \"test\",  rc=10, S=40)\n",
    "\n",
    "pin = torch.cuda.is_available()\n",
    "g = torch.Generator(); g.manual_seed(42)\n",
    "\n",
    "train_dl_c = DataLoader(train_ds_c, batch_size=128, shuffle=True,  pin_memory=pin, num_workers=0)\n",
    "valid_dl_c = DataLoader(valid_ds_c, batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n",
    "test_dl_c  = DataLoader(test_ds_c,  batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n",
    "\n",
    "# Optional: class weights for imbalance (computed from training labels)\n",
    "from collections import Counter\n",
    "labels_train = []\n",
    "for _, _, y in DataLoader(train_ds_c, batch_size=1024, shuffle=False, num_workers=0):\n",
    "    labels_train.extend(y.numpy().tolist())\n",
    "cnt = Counter(labels_train)\n",
    "num_classes = 3\n",
    "total = sum(cnt.values())\n",
    "weights = torch.tensor([total / max(cnt.get(c,1),1) for c in range(num_classes)], dtype=torch.float32)\n",
    "print(\"Class counts:\", cnt, \" -> weights:\", weights.tolist())\n",
    "\n",
    "# ---- Cell: init and train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clf = DeepCoNNClassifier(\n",
    "    glove_dim=train_ds_c.H,  # MUST equal cached sentence dim (e.g., 100 for GloVe 6B-100d)\n",
    "    rc=10,\n",
    "    backbone=\"prajjwal1/bert-tiny\",\n",
    "    proj_dim=128,\n",
    "    dropout=0.2,\n",
    "    freeze_backbone=False,\n",
    "    offline_only=True,\n",
    "    num_classes=3,\n",
    "    share_backbone=False\n",
    ").to(device)\n",
    "\n",
    "best_cls_path = os.path.join(MODEL_DIR, \"best_model_cls.pt\")\n",
    "train_loop_cls(\n",
    "    train_dl_c, valid_dl_c, clf, device,\n",
    "    epochs=5, lr=2e-3, patience=2,\n",
    "    model_path=best_cls_path,\n",
    "    class_weights=weights\n",
    ")\n",
    "\n",
    "# ---- Cell: load best, evaluate on test, print report, plot confusion matrix\n",
    "best_clf = DeepCoNNClassifier(\n",
    "    glove_dim=train_ds_c.H, rc=10, backbone=\"prajjwal1/bert-tiny\",\n",
    "    proj_dim=128, dropout=0.2, freeze_backbone=False, offline_only=True,\n",
    "    num_classes=3, share_backbone=False\n",
    ").to(device)\n",
    "best_clf.load_state_dict(torch.load(best_cls_path, map_location=device))\n",
    "\n",
    "test_acc, y_true, y_pred = evaluate_cls(best_clf, test_dl_c, device, desc=\"Test\")\n",
    "print(f\"Test Accuracy = {test_acc:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "target_names = [\"negative\",\"neutral\",\"positive\"]\n",
    "report = classification_report(y_true, y_pred, target_names=target_names, digits=4)\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "with open(os.path.join(FIG_DIR, \"classification_report.txt\"), \"w\") as f:\n",
    "    f.write(f\"Test Accuracy: {test_acc:.4f}\\n\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "# Confusion matrix (raw and normalized) + save figures\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n",
    "plot_confusion_matrix(cm, classes=target_names, normalize=False,\n",
    "                      title=\"Confusion Matrix (Counts)\",\n",
    "                      fname=os.path.join(FIG_DIR, \"confusion_matrix_counts.png\"))\n",
    "plot_confusion_matrix(cm, classes=target_names, normalize=True,\n",
    "                      title=\"Confusion Matrix (Row-Normalized)\",\n",
    "                      fname=os.path.join(FIG_DIR, \"confusion_matrix_normalized.png\"))\n",
    "\n",
    "print(\"Saved:\",\n",
    "      os.path.join(FIG_DIR, \"classification_report.txt\"), \",\",\n",
    "      os.path.join(FIG_DIR, \"confusion_matrix_counts.png\"), \",\",\n",
    "      os.path.join(FIG_DIR, \"confusion_matrix_normalized.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 715814,
     "sourceId": 1246668,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8421750,
     "sourceId": 13288272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8069290,
     "sourceId": 12764555,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
