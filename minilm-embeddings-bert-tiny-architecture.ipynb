{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814},{"sourceId":12764555,"sourceType":"datasetVersion","datasetId":8069290},{"sourceId":13288272,"sourceType":"datasetVersion","datasetId":8421750},{"sourceId":13643607,"sourceType":"datasetVersion","datasetId":8672903}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:45:08.637622Z","iopub.execute_input":"2025-11-29T11:45:08.638228Z","iopub.status.idle":"2025-11-29T11:45:08.672626Z","shell.execute_reply.started":"2025-11-29T11:45:08.638202Z","shell.execute_reply":"2025-11-29T11:45:08.671921Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/config.json\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/tokenizer.json\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/tokenizer_config.json\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/pytorch_model.bin\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/model.safetensors\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/special_tokens_map.json\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/vocab.txt\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/.gitignore\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/tokenizer_config.json.lock\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/vocab.txt.lock\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/tokenizer.json.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/tokenizer.json.lock\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/tokenizer_config.json.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/pytorch_model.bin.lock\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/special_tokens_map.json.lock\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/vocab.txt.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/config.json.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/pytorch_model.bin.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/model.safetensors.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/special_tokens_map.json.metadata\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/model.safetensors.lock\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2/.cache/huggingface/download/config.json.lock\n/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\n/kaggle/input/all-beauty/All_Beauty.jsonl\n/kaggle/input/digital-music-5/punctuations.txt\n/kaggle/input/digital-music-5/stopwords.txt\n/kaggle/input/digital-music-5/Digital_Music_5.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Standard libs\nimport os, sys, time, math, json, inspect, argparse, random, re\nfrom pathlib import Path\n\n# PyData / ML\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Tokenization (no extra downloads required)\nfrom nltk.tokenize import WordPunctTokenizer\n\n# Hugging Face datasets\ntry:\n    from datasets import load_dataset\nexcept Exception as e:\n    raise RuntimeError(\n        \"Please install 'datasets' first, e.g. `pip install datasets`\"\n    ) from e\n\n# ---- Reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nset_seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:45:08.673804Z","iopub.execute_input":"2025-11-29T11:45:08.674019Z","iopub.status.idle":"2025-11-29T11:45:08.680923Z","shell.execute_reply.started":"2025-11-29T11:45:08.673999Z","shell.execute_reply":"2025-11-29T11:45:08.680285Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(\"CUDA available:\", torch.cuda.is_available())\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:45:08.681576Z","iopub.execute_input":"2025-11-29T11:45:08.681781Z","iopub.status.idle":"2025-11-29T11:45:08.770747Z","shell.execute_reply.started":"2025-11-29T11:45:08.681757Z","shell.execute_reply":"2025-11-29T11:45:08.769996Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pathlib import Path\n\nIN_KAGGLE = Path(\"/kaggle\").exists()\n\nif IN_KAGGLE:\n    INPUT_ROOT = Path(\"/kaggle/input\")\n    WORK_ROOT  = Path(\"/kaggle/working\")\n\n    STOPWORDS = INPUT_ROOT / \"digital-music-5\" / \"stopwords.txt\"\n    PUNCTS    = INPUT_ROOT / \"digital-music-5\" / \"punctuations.txt\"\n    GLOVE     = INPUT_ROOT / \"glove6b100dtxt\" / \"glove.6B.100d.txt\"\n\n    RAW_ALL_BEAUTY = INPUT_ROOT / \"all-beauty\" / \"All_Beauty.jsonl\"\n    LOCAL_ST_MODEL = INPUT_ROOT / \"minilm-l6-v2-local\" / \"all-MiniLM-L6-v2\"\nelse:\n    PROJECT_ROOT = Path.cwd()\n\n    # if the folders are directly under the project root:\n    RAW_ALL_BEAUTY = PROJECT_ROOT / \"All_Beauty\" / \"All_Beauty.jsonl\"\n    GLOVE          = PROJECT_ROOT / \"glove.6B.100d.txt\" / \"glove.6B.100d.txt\"\n    STOPWORDS      = PROJECT_ROOT / \"Digital_Music_5\" / \"stopwords.txt\"\n    PUNCTS         = PROJECT_ROOT / \"Digital_Music_5\" / \"punctuations.txt\"\n    # pointing to the directory that contains config.json, tokenizer.json, etc.\n    LOCAL_ST_MODEL = PROJECT_ROOT / \"all-MiniLM-L6-v2\" / \"all-MiniLM-L6-v2\"\n\n    ARTIFACTS = PROJECT_ROOT / \"artifacts\"\n\n# Output directories as Path\nWORK_DIR      = WORK_ROOT if IN_KAGGLE else ARTIFACTS\nSAVE_DATA_DIR = WORK_DIR / \"Amazon_Fashion\"\nMODEL_DIR     = WORK_DIR / \"model\"\nFIG_DIR       = WORK_DIR / \"fig\"\nST_CACHE_DIR  = WORK_DIR / \"st_cache\"\n\n# Create directories\nfor d in [SAVE_DATA_DIR, MODEL_DIR, FIG_DIR, ST_CACHE_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:45:33.845394Z","iopub.execute_input":"2025-11-29T11:45:33.846138Z","iopub.status.idle":"2025-11-29T11:45:33.852180Z","shell.execute_reply.started":"2025-11-29T11:45:33.846106Z","shell.execute_reply":"2025-11-29T11:45:33.851510Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"for p in [RAW_ALL_BEAUTY, GLOVE, STOPWORDS, PUNCTS, LOCAL_ST_MODEL]:\n    print(p, \"exists:\", p.exists())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:46:04.656169Z","iopub.execute_input":"2025-11-29T11:46:04.656441Z","iopub.status.idle":"2025-11-29T11:46:04.664556Z","shell.execute_reply.started":"2025-11-29T11:46:04.656420Z","shell.execute_reply":"2025-11-29T11:46:04.663896Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/all-beauty/All_Beauty.jsonl exists: True\n/kaggle/input/glove6b100dtxt/glove.6B.100d.txt exists: True\n/kaggle/input/digital-music-5/stopwords.txt exists: True\n/kaggle/input/digital-music-5/punctuations.txt exists: True\n/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2 exists: True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom datasets import load_dataset  # optional fallback\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import WordPunctTokenizer\n\n\ndef load_all_beauty_local(jsonl_path: str | Path | None = None):\n    \"\"\"\n    Load All_Beauty.jsonl (Kaggle or local) and return a pandas DataFrame\n    with standardized columns: userID, itemID, review, rating.\n    \"\"\"\n    if jsonl_path is None:\n        jsonl_path = RAW_ALL_BEAUTY\n    jsonl_path = Path(jsonl_path)\n\n    needed = (\"user_id\", \"asin\", \"text\", \"rating\")\n\n    def _standardize_cols(df):\n        alt_map = {\n            \"reviewText\": \"text\",\n            \"overall\": \"rating\",\n            \"user\": \"user_id\",\n            \"item\": \"asin\",\n        }\n        for old, new in alt_map.items():\n            if old in df.columns and new not in df.columns:\n                df[new] = df[old]\n\n        missing = [c for c in needed if c not in df.columns]\n        if missing:\n            raise KeyError(\n                f\"Missing required columns {missing}. \"\n                \"Make sure your JSONL has keys like: user_id, asin, text, rating.\"\n            )\n\n        df = df[list(needed)].copy()\n        df.columns = [\"userID\", \"itemID\", \"review\", \"rating\"]\n        df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\")\n        df = df[df[\"rating\"].notnull()]\n        df = df[df[\"review\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n        df.reset_index(drop=True, inplace=True)\n        return df\n\n\n\n    try:\n        df = pd.read_json(jsonl_path, lines=True)\n        return _standardize_cols(df)\n    except Exception as e_pd:\n        try:\n            ds = load_dataset(\"json\", data_files=str(jsonl_path), split=\"train\")\n            df = ds.to_pandas()\n            return _standardize_cols(df)\n        except Exception as e_hf:\n            raise RuntimeError(\n                f\"Failed to load JSONL via pandas ({type(e_pd).__name__}: {e_pd}) \"\n                f\"and datasets ({type(e_hf).__name__}: {e_hf}).\"\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:46:12.690830Z","iopub.execute_input":"2025-11-29T11:46:12.691577Z","iopub.status.idle":"2025-11-29T11:46:12.699594Z","shell.execute_reply.started":"2025-11-29T11:46:12.691547Z","shell.execute_reply":"2025-11-29T11:46:12.698900Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- keep your helpers as-is ---\ndef _read_list(path):\n    if not os.path.exists(path):\n        raise FileNotFoundError(\n            f\"Required file not found: {path}\\n\"\n            \"Place a plain-text file with one token per line.\"\n        )\n    with open(path, encoding=\"utf-8\") as f:\n        return set(ln.strip() for ln in f if ln.strip())\n\ndef process_df_to_csv(df, stopwords_path, puncts_path, train_rate, csv_path):\n    # Map IDs to contiguous integers\n    df[\"userID\"] = df[\"userID\"].astype(\"category\").cat.codes\n    df[\"itemID\"] = df[\"itemID\"].astype(\"category\").cat.codes\n\n    # Load stopwords/punctuations\n    stop_words   = _read_list(stopwords_path)\n    punctuations = _read_list(puncts_path)\n    tok = WordPunctTokenizer()\n\n    def clean_review(review: str) -> str:\n        rv = review.lower()\n        for p in punctuations:\n            rv = rv.replace(p, \" \")\n        toks = tok.tokenize(rv)\n        toks = [w for w in toks if w not in stop_words]\n        return \" \".join(toks)\n\n    print(\"#### Cleaning text (this can take a while on large splits)...\")\n    df[\"review\"] = df[\"review\"].apply(clean_review)\n\n    # Train/valid/test split\n    train_df, valid_test_df = train_test_split(df, test_size=1 - train_rate, random_state=3)\n    valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=4)\n\n    os.makedirs(csv_path, exist_ok=True)\n    train_df.to_csv(os.path.join(csv_path, \"train.csv\"), index=False, header=False)\n    valid_df.to_csv(os.path.join(csv_path, \"valid.csv\"), index=False, header=False)\n    test_df .to_csv(os.path.join(csv_path, \"test.csv\"),  index=False, header=False)\n\n    print(f\"#### Saved CSVs to {csv_path}\")\n    print(f\"#### Split sizes: train {len(train_df)}, valid {len(valid_df)}, test {len(test_df)}\")\n    print(f\"#### Totals: {len(df)} reviews, {df['userID'].nunique()} users, {df['itemID'].nunique()} items.\")\n    return train_df, valid_df, test_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:46:28.259071Z","iopub.execute_input":"2025-11-29T11:46:28.259359Z","iopub.status.idle":"2025-11-29T11:46:28.267610Z","shell.execute_reply.started":"2025-11-29T11:46:28.259336Z","shell.execute_reply":"2025-11-29T11:46:28.266886Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"csv_train = SAVE_DATA_DIR / \"train.csv\"\n\nif not csv_train.exists():\n    df_raw = load_all_beauty_local()  # uses Kaggle or local automatically\n    _ = process_df_to_csv(\n        df_raw,\n        stopwords_path=STOPWORDS,\n        puncts_path=PUNCTS,\n        train_rate=0.8,\n        csv_path=SAVE_DATA_DIR,\n    )\nelse:\n    print(\"CSV files already exist â€” skipping reprocessing.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:46:38.433682Z","iopub.execute_input":"2025-11-29T11:46:38.434347Z","iopub.status.idle":"2025-11-29T11:47:07.094040Z","shell.execute_reply.started":"2025-11-29T11:46:38.434322Z","shell.execute_reply":"2025-11-29T11:47:07.093349Z"}},"outputs":[{"name":"stdout","text":"#### Cleaning text (this can take a while on large splits)...\n#### Saved CSVs to /kaggle/working/Amazon_Fashion\n#### Split sizes: train 560646, valid 70081, test 70081\n#### Totals: 700808 reviews, 631352 users, 115637 items.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def now(f='%Y-%m-%d %H:%M:%S'):\n    return time.strftime(f, time.localtime())\n\nclass Config:\n    # Device\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    # Training\n    train_epochs        = 5\n    batch_size          = 128\n    learning_rate       = 2e-3\n    l2_regularization   = 1e-6\n    learning_rate_decay = 0.99\n    patience            = 3\n\n    # Files\n    word2vec_file = GLOVE\n    train_file    = os.path.join(SAVE_DATA_DIR, 'train.csv')\n    valid_file    = os.path.join(SAVE_DATA_DIR, 'valid.csv')\n    test_file     = os.path.join(SAVE_DATA_DIR, 'test.csv')\n    model_file    = os.path.join(MODEL_DIR, 'best_model.pt')\n\n    # Data shaping\n    review_count         = 10    # number of reviews per side\n    review_length        = 40    # tokens per review\n    lowest_review_count  = 2\n    PAD_WORD             = '<UNK>'\n\n    # Model sizes\n    kernel_count = 100\n    kernel_size  = 3\n    dropout_prob = 0.5\n    cnn_out_dim  = 50\n\n    def __init__(self):\n        # Allow CLI/nb override (no-op by default)\n        attributes = inspect.getmembers(self, lambda a: not inspect.isfunction(a))\n        attributes = list(filter(lambda x: not x[0].startswith('__'), attributes))\n        parser = argparse.ArgumentParser(add_help=False)\n        for key, val in attributes:\n            parser.add_argument('--' + key, dest=key, type=type(val), default=val)\n        args, _ = parser.parse_known_args([])\n        for key, val in args.__dict__.items():\n            setattr(self, key, val)\n\n    def __str__(self):\n        attributes = inspect.getmembers(self, lambda a: not inspect.isfunction(a))\n        attributes = list(filter(lambda x: not x[0].startswith('__'), attributes))\n        return \"\\n\".join([f\"{k} = {v}\" for k, v in attributes])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:55:34.796397Z","iopub.execute_input":"2025-11-29T11:55:34.797365Z","iopub.status.idle":"2025-11-29T11:55:34.804552Z","shell.execute_reply.started":"2025-11-29T11:55:34.797339Z","shell.execute_reply":"2025-11-29T11:55:34.803904Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Cell 1: imports & small utils\nimport os, json, re, csv, hashlib\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm.auto import tqdm\n\ndef ensure_dir(p: Path):\n    p.mkdir(parents=True, exist_ok=True)\n\ndef simple_sentence_split(text: str, max_sentences: int):\n    \"\"\"Split on . ! ? and keep up to max_sentences.\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return []\n    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n    sents = [s.strip() for s in parts if s.strip()]\n    return sents[:max_sentences]\n\ndef sha1(s: str) -> str:\n    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:56:12.242922Z","iopub.execute_input":"2025-11-29T11:56:12.243238Z","iopub.status.idle":"2025-11-29T11:56:12.249055Z","shell.execute_reply.started":"2025-11-29T11:56:12.243215Z","shell.execute_reply":"2025-11-29T11:56:12.248306Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def read_split(csv_path: str):\n    \"\"\"Reads CSV with no header: userID,itemID,review,rating\"\"\"\n    rows = []\n    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n        rdr = csv.reader(f)\n        for user_id, item_id, review, rating in rdr:\n            rows.append((int(user_id), int(item_id), review, float(rating)))\n    return rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:56:14.998757Z","iopub.execute_input":"2025-11-29T11:56:14.999028Z","iopub.status.idle":"2025-11-29T11:56:15.003743Z","shell.execute_reply.started":"2025-11-29T11:56:14.999007Z","shell.execute_reply":"2025-11-29T11:56:15.002948Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def precompute_sent_embeddings(\n    train_csv: str,\n    valid_csv: str,\n    test_csv: str,\n    out_dir: str,\n    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n    S: int = 40,\n    batch_size: int = 1024,\n    max_len: int = 128,\n    lowercase: bool = False,\n    local_model_dir: str | None = \"/kaggle/input/minilm-l6-v2-local/all-MiniLM-L6-v2\",  # <-- your local path\n):\n    \"\"\"\n    Notebook version of precompute_sent_embeddings.py\n    Writes:\n      out_dir/\n        embeddings.npy\n        sentences.jsonl\n        review2sent_ids.jsonl\n        splits/{train,valid,test}/ (rows.jsonl, user_reviews.json, item_reviews.json)\n    \"\"\"\n    out_dir = Path(out_dir)\n    ensure_dir(out_dir)\n    splits_dir = out_dir / \"splits\"\n    ensure_dir(splits_dir)\n\n    # 1) Read all splits\n    split_paths = {\"train\": Path(train_csv), \"valid\": Path(valid_csv), \"test\": Path(test_csv)}\n    splits = {k: read_split(str(v)) for k, v in split_paths.items()}\n\n    # 2) Collect unique reviews + per-split manifests\n    all_reviews = {}   # review_hash -> raw_review_text\n    split_rows = {}    # split -> list[dict]\n    user_reviews = {}  # split -> {userID: [review_hash,...]}\n    item_reviews = {}  # split -> {itemID: [review_hash,...]}\n\n    for split, rows in splits.items():\n        s_rows = []\n        u_map = defaultdict(list)\n        i_map = defaultdict(list)\n        for (u, it, review, rating) in rows:\n            rtxt = review.lower() if lowercase else review\n            r_hash = sha1(rtxt)\n            all_reviews.setdefault(r_hash, rtxt)\n            s_rows.append({\"userID\": u, \"itemID\": it, \"rating\": rating, \"review_hash\": r_hash})\n            u_map[u].append(r_hash)\n            i_map[it].append(r_hash)\n        split_rows[split] = s_rows\n        user_reviews[split] = {str(k): v for k, v in u_map.items()}\n        item_reviews[split] = {str(k): v for k, v in i_map.items()}\n\n    # 3) Build deduped sentence inventory (cap to S per review for downstream speed)\n    sentence_to_id = {}\n    sentences = []  # index -> text\n    review_to_sentids = {}  # review_hash -> fixed-length [int] of len S, with -1 as PAD\n\n    def get_sent_id(s):\n        if s not in sentence_to_id:\n            sentence_to_id[s] = len(sentences)\n            sentences.append(s)\n        return sentence_to_id[s]\n\n    for r_hash, rtxt in all_reviews.items():\n        sents = simple_sentence_split(rtxt, S)\n        ids = [get_sent_id(s) for s in sents]\n        ids = (ids + [-1] * (S - len(ids))) if len(ids) < S else ids[:S]\n        review_to_sentids[r_hash] = ids\n\n    print(f\"[build] Unique reviews: {len(all_reviews):,}\")\n    print(f\"[build] Unique sentences: {len(sentences):,}\")\n\n    # 4) Encode all unique sentences (with progress bar)\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Prefer local directory (offline-safe)\n    try:\n        if local_model_dir is None:\n            raise FileNotFoundError(\"No local_model_dir provided.\")\n        # quick sanity: require at least model + tokenizer files\n        needed = [\"tokenizer.json\", \"special_tokens_map.json\"]\n        missing = [f for f in needed if not os.path.exists(os.path.join(local_model_dir, f))]\n        if missing:\n            raise FileNotFoundError(f\"Missing in local model dir: {missing}\")\n        tok = AutoTokenizer.from_pretrained(local_model_dir, local_files_only=True)\n        enc = AutoModel.from_pretrained(local_model_dir, local_files_only=True).to(device)\n    except Exception as e:\n        # Optional: fallback to hub if you ever run with internet\n        print(\"[warn] Local model load failed, attempting hub:\", e)\n        tok = AutoTokenizer.from_pretrained(model_name)\n        enc = AutoModel.from_pretrained(model_name).to(device)\n   \n    \n    enc.eval()\n    for p in enc.parameters():\n        p.requires_grad = False\n\n    H = enc.config.hidden_size\n    N = len(sentences)\n    EMB = np.memmap(out_dir / \"embeddings.npy\", dtype=\"float32\", mode=\"w+\", shape=(N, H))\n\n    def mean_pool(last_hidden_state, attention_mask):\n        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n        summed = (last_hidden_state * mask).sum(dim=1)\n        counts = mask.sum(dim=1).clamp(min=1.0)\n        return summed / counts\n\n    with torch.no_grad():\n        rng = range(0, N, batch_size)\n        for start in tqdm(rng, desc=\"Encoding sentences\", unit=\"batch\"):\n            end = min(start + batch_size, N)\n            batch_texts = sentences[start:end]\n            batch = tok(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=max_len,\n                return_tensors=\"pt\",\n            ).to(device)\n            out = enc(**batch)\n            pooled = mean_pool(out.last_hidden_state, batch[\"attention_mask\"]).detach().cpu().numpy().astype(\"float32\")\n            EMB[start:end, :] = pooled\n\n    # Flush memmap\n    del EMB\n    print(\"[encode] embeddings.npy written.\")\n\n    # 5) Write metadata files\n    with open(out_dir / \"sentences.jsonl\", \"w\", encoding=\"utf-8\") as f:\n        for sid, txt in enumerate(sentences):\n            f.write(json.dumps({\"sent_id\": sid, \"text\": txt}, ensure_ascii=False) + \"\\n\")\n\n    with open(out_dir / \"review2sent_ids.jsonl\", \"w\", encoding=\"utf-8\") as f:\n        for r_hash, ids in review_to_sentids.items():\n            f.write(json.dumps({\"review_hash\": r_hash, \"sent_ids\": ids}) + \"\\n\")\n\n    # Per-split artifacts\n    for split in [\"train\", \"valid\", \"test\"]:\n        sp = splits_dir / split\n        ensure_dir(sp)\n        with open(sp / \"rows.jsonl\", \"w\", encoding=\"utf-8\") as f:\n            for row in split_rows[split]:\n                f.write(json.dumps(row) + \"\\n\")\n        with open(sp / \"user_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(user_reviews[split], f)\n        with open(sp / \"item_reviews.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(item_reviews[split], f)\n\n    print(f\"[done] Wrote cache to: {out_dir.resolve()}\")\n    print(\"      Files:\")\n    print(\"       - embeddings.npy\")\n    print(\"       - sentences.jsonl\")\n    print(\"       - review2sent_ids.jsonl\")\n    print(\"       - splits/*/rows.jsonl, user_reviews.json, item_reviews.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:56:32.210794Z","iopub.execute_input":"2025-11-29T11:56:32.211071Z","iopub.status.idle":"2025-11-29T11:56:32.228163Z","shell.execute_reply.started":"2025-11-29T11:56:32.211049Z","shell.execute_reply":"2025-11-29T11:56:32.227230Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"TRAIN_CSV = SAVE_DATA_DIR / \"train.csv\"\nVALID_CSV = SAVE_DATA_DIR / \"valid.csv\"\nTEST_CSV  = SAVE_DATA_DIR / \"test.csv\"\nOUT_DIR   = ST_CACHE_DIR\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:56:39.665611Z","iopub.execute_input":"2025-11-29T11:56:39.666303Z","iopub.status.idle":"2025-11-29T11:56:39.670424Z","shell.execute_reply.started":"2025-11-29T11:56:39.666270Z","shell.execute_reply":"2025-11-29T11:56:39.669505Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"precompute_sent_embeddings(\n    train_csv=str(TRAIN_CSV),\n    valid_csv=str(VALID_CSV),\n    test_csv=str(TEST_CSV),\n    out_dir=str(OUT_DIR),\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    S=40,\n    batch_size=1024,\n    max_len=128,\n    lowercase=True,\n    local_model_dir=str(LOCAL_ST_MODEL) if LOCAL_ST_MODEL is not None else None,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:56:48.088552Z","iopub.execute_input":"2025-11-29T11:56:48.089313Z","iopub.status.idle":"2025-11-29T12:09:39.356004Z","shell.execute_reply.started":"2025-11-29T11:56:48.089286Z","shell.execute_reply":"2025-11-29T12:09:39.355185Z"}},"outputs":[{"name":"stdout","text":"[build] Unique reviews: 618,608\n[build] Unique sentences: 618,607\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-29 11:57:04.169134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764417424.352059      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764417424.406552      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"Encoding sentences:   0%|          | 0/605 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f59793dc937e4d3dbf4bb163828f537d"}},"metadata":{}},{"name":"stdout","text":"[encode] embeddings.npy written.\n[done] Wrote cache to: /kaggle/working/st_cache\n      Files:\n       - embeddings.npy\n       - sentences.jsonl\n       - review2sent_ids.jsonl\n       - splits/*/rows.jsonl, user_reviews.json, item_reviews.json\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 4: STCachedDataset (fixed rc/S) for fast numeric I/O training\nimport json, numpy as np, torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\nclass STCachedDataset(Dataset):\n    \"\"\"\n    Loads numeric cache produced by precompute_sent_embeddings (fixed rc/S view).\n    - embeddings.npy  -> (N_sentences, H) float32 (memmap)\n    - review2sent_ids.jsonl -> review_hash -> [S] sentence ids (-1 pad)\n    - splits/{split}/rows.jsonl\n    - splits/{split}/user_reviews.json, item_reviews.json\n    \"\"\"\n    def __init__(self, cache_dir, split, rc=10, S=40):\n        self.cache_dir = Path(cache_dir)\n        self.split = split\n        self.rc = rc\n        self.S = S\n\n        # embeddings\n        self.emb = np.memmap(self.cache_dir / \"embeddings.npy\", dtype=\"float32\", mode=\"r\")\n        # discover H\n        N = self.emb.size\n        with open(self.cache_dir / \"sentences.jsonl\", \"r\", encoding=\"utf-8\") as f:\n            n_sent = sum(1 for _ in f)\n        H = N // n_sent\n        self.emb = self.emb.reshape(n_sent, H)\n        self.H = H\n\n        # review -> sent_ids\n        self.rev2ids = {}\n        with open(self.cache_dir / \"review2sent_ids.jsonl\", \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                obj = json.loads(line)\n                self.rev2ids[obj[\"review_hash\"]] = obj[\"sent_ids\"]\n\n        # rows\n        self.rows = []\n        with open(self.cache_dir / \"splits\" / split / \"rows.jsonl\", \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                self.rows.append(json.loads(line))\n\n        # groupings\n        with open(self.cache_dir / \"splits\" / split / \"user_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n            self.user_map = {int(k): v for k, v in json.load(f).items()}\n        with open(self.cache_dir / \"splits\" / split / \"item_reviews.json\", \"r\", encoding=\"utf-8\") as f:\n            self.item_map = {int(k): v for k, v in json.load(f).items()}\n\n    def _review_tensor(self, review_hashes):\n        \"\"\"\n        Build (rc, S, H) from a list of review hashes.\n        Takes the first rc; pads with zeros if fewer.\n        \"\"\"\n        chosen = (review_hashes[:self.rc] +\n                  [\"<PAD>\"] * max(0, self.rc - len(review_hashes)))\n        out = np.zeros((self.rc, self.S, self.H), dtype=np.float32)\n        for i, rh in enumerate(chosen):\n            if rh == \"<PAD>\":\n                continue\n            ids = self.rev2ids.get(rh, [-1]*self.S)\n            ids = ids[:self.S] if len(ids) >= self.S else ids + [-1]*(self.S-len(ids))\n            valid_mask = np.array(ids) >= 0\n            if valid_mask.any():\n                out[i, valid_mask, :] = self.emb[np.array(ids)[valid_mask]]\n        return torch.from_numpy(out)\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, idx):\n        row = self.rows[idx]\n        u, it, rating, rh = row[\"userID\"], row[\"itemID\"], row[\"rating\"], row[\"review_hash\"]\n        u_tensor = self._review_tensor(self.user_map.get(u, [rh]))\n        i_tensor = self._review_tensor(self.item_map.get(it, [rh]))\n        return u_tensor, i_tensor, torch.tensor([rating], dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:11:31.730664Z","iopub.execute_input":"2025-11-29T12:11:31.731435Z","iopub.status.idle":"2025-11-29T12:11:31.743668Z","shell.execute_reply.started":"2025-11-29T12:11:31.731410Z","shell.execute_reply":"2025-11-29T12:11:31.742979Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Cell 5: quick test that cache & dataset load correctly\nfrom torch.utils.data import DataLoader\n\ncache_dir = OUT_DIR  # from Cell 3\ntrain_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\ndl = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n\nbatch = next(iter(dl))\nu, i, r = batch\nprint(\"User batch:\", tuple(u.shape))  # (B, rc, S, H)\nprint(\"Item batch:\", tuple(i.shape))\nprint(\"Ratings  :\", tuple(r.shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:11:43.591293Z","iopub.execute_input":"2025-11-29T12:11:43.591594Z","iopub.status.idle":"2025-11-29T12:11:51.164941Z","shell.execute_reply.started":"2025-11-29T12:11:43.591573Z","shell.execute_reply":"2025-11-29T12:11:51.164210Z"}},"outputs":[{"name":"stdout","text":"User batch: (8, 10, 40, 384)\nItem batch: (8, 10, 40, 384)\nRatings  : (8, 1)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass FactorizationMachine(nn.Module):\n    def __init__(self, in_dim, k):\n        super().__init__()\n        self.linear = nn.Linear(in_dim, 1)\n        self.V = nn.Parameter(torch.randn(in_dim, k) * 0.01)\n    def forward(self, x):\n        linear = self.linear(x)\n        xv  = x @ self.V\n        x2v2 = (x**2) @ (self.V**2)\n        pairwise = 0.5 * (xv**2 - x2v2).sum(dim=1, keepdim=True)\n        return linear + pairwise\n\nclass CNNOverSentences(nn.Module):\n    def __init__(self, emb_dim, kernel_count=100, kernel_size=3, dropout=0.5, pool=\"max\"):\n        super().__init__()\n        self.conv = nn.Conv1d(emb_dim, kernel_count, kernel_size, padding=(kernel_size-1)//2)\n        self.act  = nn.ReLU()\n        self.drop = nn.Dropout(dropout)\n        self.pool = pool\n    def forward(self, x, mask=None):\n        # x: (B*rc, S, H)\n        z = self.conv(x.permute(0,2,1))     # (B*rc, K, S)\n        z = self.act(z).transpose(1,2)      # (B*rc, S, K)\n        if self.pool == \"max\":\n            out = z.max(dim=1).values       # (B*rc, K)\n        else:\n            out = z.mean(dim=1)\n        return self.drop(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:11:59.348491Z","iopub.execute_input":"2025-11-29T12:11:59.348990Z","iopub.status.idle":"2025-11-29T12:11:59.356008Z","shell.execute_reply.started":"2025-11-29T12:11:59.348965Z","shell.execute_reply":"2025-11-29T12:11:59.355318Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# === New model: HF backbone over cached MiniLM sentence embeddings ===\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\nclass HFBackboneOverMiniLM(nn.Module):\n    \"\"\"\n    Wrap a small HF encoder (e.g., 'prajjwal1/bert-tiny') and feed cached\n    MiniLM sentence embeddings as `inputs_embeds`.\n\n    Expects inputs:\n      u: (B, rc, S, H)  sentence-embedding tensor for users\n      i: (B, rc, S, H)  sentence-embedding tensor for items\n    Produces:\n      rating in [1, 5]\n    \"\"\"\n    def __init__(\n        self,\n        emb_dim: int,          # H from your cache (MiniLM = 384)\n        rc: int = 10,\n        S: int = 40,\n        backbone_name: str = \"prajjwal1/bert-tiny\",\n        side_out_dim: int = 128,\n        head_hidden: int = 64,\n        dropout: float = 0.1,\n        freeze_backbone: bool = False,\n    ):\n        super().__init__()\n        self.rc = rc\n        self.S  = S\n\n        # 1) Load a small backbone (tiny BERT by default)\n        self.backbone = AutoModel.from_pretrained(backbone_name)\n        self.d_model  = self.backbone.config.hidden_size  # typically 128 for bert-tiny\n\n        # Optionally freeze it (useful on small GPUs)\n        if freeze_backbone:\n            for p in self.backbone.parameters():\n                p.requires_grad = False\n\n        # 2) Project MiniLM sentence embeddings -> backbone hidden size\n        self.proj = nn.Linear(emb_dim, self.d_model)\n\n        # 3) Side encoders: run backbone over sentences per review, pool, aggregate over rc\n        self.drop = nn.Dropout(dropout)\n        self.side_proj = nn.Linear(rc * self.d_model, side_out_dim)\n\n        # 4) Final head (concat user & item)\n        self.head = nn.Sequential(\n            nn.Linear(2 * side_out_dim, head_hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(head_hidden, 1),\n        )\n\n    @staticmethod\n    def _masked_mean(x, mask, dim=1, eps=1e-8):\n        # x: (N, L, D), mask: (N, L) with 1 for valid\n        denom = mask.sum(dim=dim).clamp(min=1.).unsqueeze(-1)  # (N,1)\n        return (x * mask.unsqueeze(-1)).sum(dim=dim) / (denom + eps)\n\n    def _encode_side(self, side_tensor):\n        \"\"\"\n        side_tensor: (B, rc, S, H)\n        Steps:\n          - Flatten to (B*rc, S, H)\n          - Build attention_mask from non-zero rows\n          - Project to d_model and run HF backbone with inputs_embeds\n          - Masked mean-pool over S\n          - Reshape to (B, rc*d_model) and project to side_out_dim\n        \"\"\"\n        B, rc, S, H = side_tensor.shape\n        x = side_tensor.reshape(B * rc, S, H)                  # (B*rc, S, H)\n\n        # Attention mask: 1 where sentence embedding is non-zero\n        with torch.no_grad():\n            sent_valid = (x.abs().sum(dim=-1) > 0).to(torch.long)  # (B*rc, S)\n\n        x = self.proj(x)                                        # -> (B*rc, S, d_model)\n        out = self.backbone(inputs_embeds=x,\n                            attention_mask=sent_valid,\n                            return_dict=True)\n        seq = out.last_hidden_state                             # (B*rc, S, d_model)\n\n        pooled = self._masked_mean(seq, sent_valid, dim=1)      # (B*rc, d_model)\n        pooled = pooled.reshape(B, rc, self.d_model).reshape(B, rc * self.d_model)\n        pooled = self.drop(pooled)\n        return self.side_proj(pooled)                           # (B, side_out_dim)\n\n    def forward(self, u, i):\n        u_enc = self._encode_side(u.float())\n        i_enc = self._encode_side(i.float())\n        z = torch.cat([u_enc, i_enc], dim=1)\n        raw = self.head(z)                                      # (B, 1)\n        # Map to rating scale [1, 5]\n        rating = torch.sigmoid(raw) * 4.0 + 1.0\n        return rating\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:12:09.893993Z","iopub.execute_input":"2025-11-29T12:12:09.894725Z","iopub.status.idle":"2025-11-29T12:12:09.911489Z","shell.execute_reply.started":"2025-11-29T12:12:09.894700Z","shell.execute_reply":"2025-11-29T12:12:09.910716Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Cell 7: Training loop with progress bars\nfrom tqdm.auto import tqdm, trange\nimport time\n\ndef mse_to_rmse(m): return float(m)**0.5\n\ndef predict_mse(model, dataloader, device, desc=\"Eval\"):\n    mse, n = 0.0, 0\n    model.eval()\n    with torch.no_grad():\n        for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n            u,i,r = u.to(device), i.to(device), r.to(device)\n            preds = model(u,i)\n            mse += F.mse_loss(preds, r, reduction=\"sum\").item()\n            n   += r.size(0)\n    return mse / max(n,1)\n\ndef predict_mae(model, dataloader, device, desc=\"Eval\"):\n    mae, n = 0.0, 0\n    model.eval()\n    with torch.no_grad():\n        for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n            u,i,r = u.to(device), i.to(device), r.to(device)\n            preds = model(u,i)\n            mae += F.l1_loss(preds, r, reduction=\"sum\").item()\n            n   += r.size(0)\n    return mae / max(n,1)\n\ndef train_loop(train_dl, valid_dl, model, device, epochs=5, lr=2e-3, patience=2, model_path=\"best.pt\"):\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n    best_loss = float(\"inf\"); bad_epochs = 0\n    for epoch in trange(epochs, desc=\"Epochs\"):\n        model.train()\n        total_loss, total_samples = 0.0, 0\n        pbar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n        for u,i,r in pbar:\n            u,i,r = u.to(device), i.to(device), r.to(device)\n            preds = model(u,i)\n            loss  = F.mse_loss(preds, r, reduction=\"sum\")\n            opt.zero_grad(); loss.backward(); opt.step()\n            total_loss   += loss.item()\n            total_samples+= r.size(0)\n            running = total_loss/max(total_samples,1)\n            pbar.set_postfix(MSE=f\"{running:.4f}\",RMSE=f\"{mse_to_rmse(running):.4f}\")\n        # validation\n        valid_mse = predict_mse(model, valid_dl, device, desc=\"Valid\")\n        print(f\"Epoch {epoch:02d} | Train RMSE {mse_to_rmse(total_loss/max(total_samples,1)):.4f} \"\n              f\"| Valid RMSE {mse_to_rmse(valid_mse):.4f}\")\n        if valid_mse < best_loss:\n            best_loss = valid_mse\n            torch.save(model.state_dict(), model_path)\n            bad_epochs = 0\n        else:\n            bad_epochs += 1\n            if bad_epochs >= patience:\n                print(\"Early stopping.\")\n                break\n    print(f\"Best valid RMSE: {mse_to_rmse(best_loss):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:12:20.868505Z","iopub.execute_input":"2025-11-29T12:12:20.869044Z","iopub.status.idle":"2025-11-29T12:12:20.879752Z","shell.execute_reply.started":"2025-11-29T12:12:20.869017Z","shell.execute_reply":"2025-11-29T12:12:20.878984Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Cell 8: train and test with cached dataset\nfrom torch.utils.data import DataLoader\n\ncache_dir = OUT_DIR  # from earlier\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\nvalid_ds = STCachedDataset(cache_dir, \"valid\", rc=10, S=40)\ntest_ds  = STCachedDataset(cache_dir, \"test\",  rc=10, S=40)\n\npin = torch.cuda.is_available()\ntrain_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  pin_memory=pin, num_workers=0)\nvalid_dl = DataLoader(valid_ds, batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\ntest_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\n\nmodel = HFBackboneOverMiniLM(\n    emb_dim=train_ds.H,      # from your cache (e.g., 384)\n    rc=10,\n    S=40,\n    backbone_name=\"prajjwal1/bert-tiny\",\n    side_out_dim=128,\n    head_hidden=64,\n    dropout=0.1,\n    freeze_backbone=False,   # set True if you need to save VRAM\n).to(device)\nbest_path = str(Path(cache_dir)/\"best_model.pt\")\n# Consider a slightly lower LR if you fine-tune the backbone:\ntrain_loop(train_dl, valid_dl, model, device, epochs=5, lr=1e-3, model_path=best_path)\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:12:32.351447Z","iopub.execute_input":"2025-11-29T12:12:32.351744Z","iopub.status.idle":"2025-11-29T14:13:07.480663Z","shell.execute_reply.started":"2025-11-29T12:12:32.351723Z","shell.execute_reply":"2025-11-29T14:13:07.479896Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6db272be081749eea06870f46bb2e9b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f4d012496e44de96b687680f460b09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ba6233c07c4709b78fffb3762b39ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train 0:   0%|          | 0/4381 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c669268750f46e8a49a836526aa5141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/548 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 00 | Train RMSE 1.0782 | Valid RMSE 1.0358\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 1:   0%|          | 0/4381 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/548 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 01 | Train RMSE 1.0418 | Valid RMSE 1.0208\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 2:   0%|          | 0/4381 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/548 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02 | Train RMSE 1.0270 | Valid RMSE 1.0101\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 3:   0%|          | 0/4381 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/548 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03 | Train RMSE 1.0164 | Valid RMSE 1.0185\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train 4:   0%|          | 0/4381 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/548 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04 | Train RMSE 1.0085 | Valid RMSE 1.0143\nEarly stopping.\nBest valid RMSE: 1.0101\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# load best and test\nbest = model\nbest.load_state_dict(torch.load(best_path, map_location=device))\n\nmse = predict_mse(best, test_dl, device, desc=\"Test\")\nmae = predict_mae(best, test_dl, device, desc=\"Test\")\nprint(f\"Test RMSE={mse_to_rmse(mse):.4f}, MAE={mae:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === New classification model: HF backbone over MiniLM sentence embeddings ===\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\nclass HFBackboneOverMiniLMForClassification(nn.Module):\n    \"\"\"\n    Same encoder idea as before, but classification head (3 classes).\n    Inputs:  u, i tensors shaped (B, rc, S, H)\n    Output:  logits shaped (B, 3)\n    \"\"\"\n    def __init__(\n        self,\n        emb_dim: int,          # MiniLM cached embedding dim (e.g., 384)\n        rc: int = 10,\n        S: int = 40,\n        backbone_name: str = \"prajjwal1/bert-tiny\",\n        side_out_dim: int = 128,\n        head_hidden: int = 64,\n        dropout: float = 0.1,\n        freeze_backbone: bool = False,\n        num_classes: int = 3,\n    ):\n        super().__init__()\n        self.rc = rc\n        self.S  = S\n        self.num_classes = num_classes\n\n        self.backbone = AutoModel.from_pretrained(backbone_name)\n        self.d_model  = self.backbone.config.hidden_size\n\n        if freeze_backbone:\n            for p in self.backbone.parameters():\n                p.requires_grad = False\n\n        self.proj = nn.Linear(emb_dim, self.d_model)           # MiniLM -> backbone dim\n        self.drop = nn.Dropout(dropout)\n        self.side_proj = nn.Linear(rc * self.d_model, side_out_dim)\n\n        self.cls_head = nn.Sequential(\n            nn.Linear(2 * side_out_dim, head_hidden),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(head_hidden, num_classes),               # logits (no softmax)\n        )\n\n    @staticmethod\n    def _masked_mean(x, mask, dim=1, eps=1e-8):\n        # x: (N, L, D), mask: (N, L) with 1=valid\n        denom = mask.sum(dim=dim).clamp(min=1.).unsqueeze(-1)\n        return (x * mask.unsqueeze(-1)).sum(dim=dim) / (denom + eps)\n\n    def _encode_side(self, side_tensor):\n        # side_tensor: (B, rc, S, H)\n        B, rc, S, H = side_tensor.shape\n        x = side_tensor.reshape(B * rc, S, H)\n\n        # attention mask from zero-padded sentence rows\n        with torch.no_grad():\n            sent_valid = (x.abs().sum(dim=-1) > 0).to(torch.long)  # (B*rc, S)\n\n        x = self.proj(x)  # (B*rc, S, d_model)\n        out = self.backbone(inputs_embeds=x, attention_mask=sent_valid, return_dict=True)\n        seq = out.last_hidden_state  # (B*rc, S, d_model)\n\n        pooled = self._masked_mean(seq, sent_valid, dim=1)  # (B*rc, d_model)\n        pooled = pooled.reshape(B, rc, self.d_model).reshape(B, rc * self.d_model)\n        pooled = self.drop(pooled)\n        return self.side_proj(pooled)  # (B, side_out_dim)\n\n    def forward(self, u, i):\n        u_enc = self._encode_side(u.float())\n        i_enc = self._encode_side(i.float())\n        z = torch.cat([u_enc, i_enc], dim=1)\n        logits = self.cls_head(z)  # (B, 3)\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Ratings -> {0:neg, 1:neutral, 2:pos} helpers ===\nimport torch\n\ndef ratings_to_classes(r_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    r_tensor: shape (B,) or (B,1), float ratings.\n    Mapping: 1,2 -> 0 | 3 -> 1 | 4,5 -> 2\n    \"\"\"\n    r = r_tensor.view(-1).detach().cpu().numpy()\n    cls = []\n    for x in r:\n        xi = int(round(float(x)))\n        if xi in (1, 2):\n            cls.append(0)\n        elif xi == 3:\n            cls.append(1)\n        else:  # 4 or 5 (and any >3)\n            cls.append(2)\n    return torch.tensor(cls, dtype=torch.long, device=r_tensor.device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Ratings -> {0:neg, 1:neutral, 2:pos} helpers ===\nimport torch\n\ndef ratings_to_classes(r_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    r_tensor: shape (B,) or (B,1), float ratings.\n    Mapping: 1,2 -> 0 | 3 -> 1 | 4,5 -> 2\n    \"\"\"\n    r = r_tensor.view(-1).detach().cpu().numpy()\n    cls = []\n    for x in r:\n        xi = int(round(float(x)))\n        if xi in (1, 2):\n            cls.append(0)\n        elif xi == 3:\n            cls.append(1)\n        else:  # 4 or 5 (and any >3)\n            cls.append(2)\n    return torch.tensor(cls, dtype=torch.long, device=r_tensor.device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Classification training/eval ===\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm, trange\n\n@torch.no_grad()\ndef predict_accuracy(model, dataloader, device, desc=\"Eval\"):\n    model.eval()\n    correct, total = 0, 0\n    for u,i,r in tqdm(dataloader, desc=desc, leave=False):\n        u,i,r = u.to(device), i.to(device), r.to(device)\n        y = ratings_to_classes(r)                 # (B,)\n        logits = model(u,i)                       # (B, 3)\n        pred = logits.argmax(dim=1)               # (B,)\n        correct += (pred == y).sum().item()\n        total   += y.numel()\n    return correct / max(total, 1)\n\ndef train_loop_cls(train_dl, valid_dl, model, device,\n                   epochs=5, lr=1e-3, patience=2, model_path=\"best.pt\"):\n    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n    best_acc, bad_epochs = 0.0, 0\n    for epoch in trange(epochs, desc=\"Epochs\"):\n        model.train()\n        running_loss, running_correct, running_total = 0.0, 0, 0\n\n        pbar = tqdm(train_dl, desc=f\"Train {epoch}\", leave=False)\n        for u,i,r in pbar:\n            u,i,r = u.to(device), i.to(device), r.to(device)\n            y = ratings_to_classes(r)                 # (B,)\n            logits = model(u,i)                       # (B,3)\n            loss = F.cross_entropy(logits, y)\n\n            opt.zero_grad(); loss.backward(); opt.step()\n\n            running_loss += loss.item() * y.size(0)\n            pred = logits.argmax(dim=1)\n            running_correct += (pred == y).sum().item()\n            running_total   += y.numel()\n\n            train_acc = running_correct / max(running_total, 1)\n            pbar.set_postfix(loss=f\"{running_loss/max(running_total,1):.4f}\",\n                             acc=f\"{train_acc:.4f}\")\n\n        # validation accuracy\n        val_acc = predict_accuracy(model, valid_dl, device, desc=\"Valid\")\n        print(f\"Epoch {epoch:02d} | Train Acc {train_acc:.4f} | Valid Acc {val_acc:.4f}\")\n\n        if val_acc > best_acc:\n            best_acc = val_acc\n            torch.save(model.state_dict(), model_path)\n            bad_epochs = 0\n        else:\n            bad_epochs += 1\n            if bad_epochs >= patience:\n                print(\"Early stopping.\")\n                break\n\n    print(f\"Best valid accuracy: {best_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Confusion matrix on TEST ===\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n@torch.no_grad()\ndef confusion_on_loader(model, dataloader, device):\n    model.eval()\n    ys, ps = [], []\n    for u,i,r in tqdm(dataloader, desc=\"Confusion/Test\", leave=False):\n        u,i,r = u.to(device), i.to(device), r.to(device)\n        y = ratings_to_classes(r).cpu().numpy()\n        logits = model(u,i)\n        p = logits.argmax(dim=1).cpu().numpy()\n        ys.append(y); ps.append(p)\n    y_true = np.concatenate(ys) if ys else np.array([], dtype=int)\n    y_pred = np.concatenate(ps) if ps else np.array([], dtype=int)\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\n    acc = (y_true == y_pred).mean() if y_true.size else 0.0\n    return cm, acc\n\ndef plot_confusion_matrix(cm, class_names=(\"neg\",\"neutral\",\"pos\"), normalize=False, title=\"Confusion Matrix\"):\n    if normalize:\n        cm = cm.astype(float)\n        row_sums = cm.sum(axis=1, keepdims=True).clip(min=1.0)\n        cm = cm / row_sums\n\n    fig, ax = plt.subplots(figsize=(5,4))\n    im = ax.imshow(cm, interpolation='nearest')  # no explicit colors\n    ax.set_title(title)\n    ax.set_xticks(range(len(class_names)))\n    ax.set_yticks(range(len(class_names)))\n    ax.set_xticklabels(class_names)\n    ax.set_yticklabels(class_names)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n\n    # Annotate cells\n    fmt = \".2f\" if normalize else \"d\"\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\")\n\n    fig.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: train and test with cached dataset\nfrom torch.utils.data import DataLoader\n\ncache_dir = OUT_DIR  # from earlier\n\ntrain_ds = STCachedDataset(cache_dir, \"train\", rc=10, S=40)\nvalid_ds = STCachedDataset(cache_dir, \"valid\", rc=10, S=40)\ntest_ds  = STCachedDataset(cache_dir, \"test\",  rc=10, S=40)\n\npin = torch.cuda.is_available()\ntrain_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  pin_memory=pin, num_workers=0)\nvalid_dl = DataLoader(valid_ds, batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)\ntest_dl  = DataLoader(test_ds,  batch_size=128, shuffle=False, pin_memory=pin, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Build classification model (same encoder params you used before) ----\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = HFBackboneOverMiniLMForClassification(\n    emb_dim=train_ds.H,\n    rc=10,\n    S=40,\n    backbone_name=\"prajjwal1/bert-tiny\",\n    side_out_dim=128,\n    head_hidden=64,\n    dropout=0.1,\n    freeze_backbone=False,\n).to(device)\n\nbest_path = str(Path(cache_dir) / \"best_model_cls.pt\")\n\n# ---- Train for classification ----\ntrain_loop_cls(train_dl, valid_dl, model, device, epochs=5, lr=1e-3, model_path=best_path)\n\n# ---- Load best and test accuracy + confusion matrix ----\nbest = HFBackboneOverMiniLMForClassification(\n    emb_dim=train_ds.H, rc=10, S=40,\n    backbone_name=\"prajjwal1/bert-tiny\",\n    side_out_dim=128, head_hidden=64, dropout=0.1,\n    freeze_backbone=False,\n).to(device)\nbest.load_state_dict(torch.load(best_path, map_location=device), strict=True)\nbest.eval()\n\ncm, test_acc = confusion_on_loader(best, test_dl, device)\nprint(f\"Test Accuracy = {test_acc:.4f}\")\n\n# Plot (counts)\nplot_confusion_matrix(cm, class_names=(\"neg\",\"neutral\",\"pos\"), normalize=False,\n                      title=\"Confusion Matrix (counts)\")\n\n# Plot (row-normalised)\nplot_confusion_matrix(cm, class_names=(\"neg\",\"neutral\",\"pos\"), normalize=True,\n                      title=\"Confusion Matrix (row-normalised)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Collect y_true / y_pred on a dataloader ===\nimport numpy as np\nimport torch\n\n@torch.no_grad()\ndef predictions_on_loader(model, dataloader, device):\n    model.eval()\n    ys, ps = [], []\n    for u,i,r in dataloader:\n        u,i,r = u.to(device), i.to(device), r.to(device)\n        y = ratings_to_classes(r).cpu().numpy()\n        logits = model(u,i)\n        p = logits.argmax(dim=1).cpu().numpy()\n        ys.append(y); ps.append(p)\n    y_true = np.concatenate(ys) if ys else np.array([], dtype=int)\n    y_pred = np.concatenate(ps) if ps else np.array([], dtype=int)\n    return y_true, y_pred\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# After loading your best classification model:\ny_true, y_pred = predictions_on_loader(best, test_dl, device)\n\n# Accuracy\ntest_acc = accuracy_score(y_true, y_pred)\nprint(f\"Test Accuracy = {test_acc:.4f}\")\n\n# Classification report\ntarget_names = [\"neg\", \"neutral\", \"pos\"]   # 0,1,2\nprint(classification_report(\n    y_true, y_pred,\n    labels=[0,1,2],\n    target_names=target_names,\n    digits=4\n))\n\n# Confusion matrix (counts + row-normalised plots)\ncm = confusion_matrix(y_true, y_pred, labels=[0,1,2])\nplot_confusion_matrix(cm, class_names=target_names, normalize=False,\n                      title=\"Confusion Matrix (counts)\")\nplot_confusion_matrix(cm, class_names=target_names, normalize=True,\n                      title=\"Confusion Matrix (row-normalised)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}